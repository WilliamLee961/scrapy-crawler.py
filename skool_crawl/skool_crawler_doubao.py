# å¸–å­å†…å®¹æŠ“å–å¹¶è°ƒç”¨è±†åŒ…1.6å¯¹æœ€æ–°20æ¡å¸–å­å½¢æˆç»¼è¿°

# Usage example:
# python skool_crawler_doubao.py --group ai-automation-society --limit 3 --storage_state skool_state.json --output_csv skool_posts.csv --output_db skool_scrape.db --summary_out summary_doubao.json --doubao_key 165e659b-a12e-462d-8398-68da89fbcebb --debug

import os
import re
import time
import json
import sqlite3
import argparse
import requests
import pandas as pd
from typing import List, Dict, Optional
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PlayTimeoutError
from volcenginesdkarkruntime import Ark

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
DEFAULT_DOUBAO_API_KEY = "165e659b-a12e-462d-8398-68da89fbcebb"
DOUBAO_API_URL = "https://ark.cn-beijing.volces.com/api/v3/chat/completion"

DEFAULT_DB = "skool_scrape.db"
DEFAULT_CSV = "skool_posts.csv"
DEFAULT_SUMMARY_JSON = "summary_doubao.json"

def now_iso():
    return datetime.now(timezone.utc).isoformat()

def ensure_dir_for_file(path: str):
    d = os.path.dirname(path)
    if d:
        os.makedirs(d, exist_ok=True)

def fetch_group_html(storage_state: str, group_slug: str, outfile: str, headless=True, max_scrolls=10, scroll_pause=3):
    """ä½¿ç”¨ Playwright ç™»å½•åæ»šåŠ¨åŠ è½½é¡µé¢"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        ctx = browser.new_context(storage_state=storage_state)
        page = ctx.new_page()

        url = f"https://www.skool.com/{group_slug}"
        print(f"è®¿é—® {url}")
        page.goto(url, timeout=60000)

        last_height = 0
        for i in range(max_scrolls):
            page.mouse.wheel(0, 20000)
            time.sleep(scroll_pause)
            height = page.evaluate("document.body.scrollHeight")
            if height == last_height:
                print(f"æ»šåŠ¨åœæ­¢ï¼šå·²è¾¾åˆ°é«˜åº¦ {height}, scroll rounds: {i+1}")
                break
            last_height = height

        html = page.content()
        with open(outfile, "w", encoding="utf-8") as f:
            f.write(html)
        print(f" å·²ä¿å­˜æ¸²æŸ“å HTML åˆ° {outfile}")

        browser.close()
        return html


def parse_posts_from_html(html: str, group_slug: str) -> List[Dict]:
    soup = BeautifulSoup(html, "lxml")
    selectors = [
        "div[class*='PostItemWrapper']",
        "div[class*='PostItemCardWrapper']",
        "div[class*='PostItemCardContent']",
        "div[class*='PostItem']",
        "div[class*='PostListWrapper']",
    ]
    posts = []
    for sel in selectors:
        nodes = soup.select(sel)
        if nodes:
            posts = nodes
            break

    if not posts:
        anchors = soup.find_all("a", href=True)
        for a in anchors:
            href = a["href"]
            text = a.get_text(strip=True)
            if href.startswith(f"/{group_slug}/") and not any(skip in href for skip in ["calendar", "classroom", "members", "?c="]):
                if len(text) > 3:
                    posts.append(a)

    results = []
    for idx, node in enumerate(posts):
        is_pinned = False
        # æ£€æŸ¥èŠ‚ç‚¹å†…æ˜¯å¦æœ‰"ç½®é¡¶"æˆ–"Pinned"æ–‡æœ¬ï¼ˆå¤šè¯­è¨€å…¼å®¹ï¼‰
        pinned_texts = node.find_all(string=re.compile(r"ç½®é¡¶|Pinned", re.I))
        if pinned_texts:
            is_pinned = True
        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰ç½®é¡¶ç›¸å…³çš„classï¼ˆå¦‚"pinned", "sticky"ï¼‰
        if "class" in node.attrs:
            node_classes = " ".join(node.attrs["class"])
            if re.search(r"pinned|sticky", node_classes, re.I):
                is_pinned = True

        # è§£ææ ‡é¢˜ï¼ŒURLç­‰       
        if node.name == "a":
            title_tag = node
            parent = node.find_parent("div") or node
        else:
            title_tag = None
            for a in node.find_all("a", href=True):
                href = a["href"]
                text = a.get_text(strip=True)
                if href.startswith(f"/{group_slug}/") and len(text) > 3 and not any(x in href for x in ["calendar", "classroom", "members", "?c="]):
                    if "-" in href or "?p=" in href or "new-video" in href:
                        title_tag = a
                        break
                    if title_tag is None:
                        title_tag = a
            parent = node

        title = title_tag.get_text(" ", strip=True) if title_tag else None
        url = ("https://www.skool.com" + title_tag["href"]) if title_tag else None

        author = None
        avatar_img = parent.select_one("div[class*='AvatarWrapper'] img") if parent else None
        if avatar_img and avatar_img.get("alt"):
            author = avatar_img.get("alt")

        time_node = parent.select_one("[class*='PostTimeContent'], [class*='PostTime']")
        time_text = time_node.get_text(" ", strip=True) if time_node else None

        excerpt_node = parent.select_one("[class*='ContentPreviewWrapper'], [class*='ContentPreview']")
        excerpt = excerpt_node.get_text(" ", strip=True) if excerpt_node else None

        results.append({
            "idx": idx,
            "title": title,
            "url": url,
            "author": author,
            "time": time_text,
            "excerpt": excerpt,
            "is_pinned": is_pinned
        })

    return results


def parse_time(text: str) -> datetime:
    """è§£æè‹±æ–‡æ—¶é—´æ–‡æœ¬ï¼ˆæ–°å¢æ”¯æŒ Jun '24 ç­‰æ ¼å¼ï¼‰ï¼Œè¿”å›UTCæ—¶é—´"""
    if not text:
        return datetime.min.replace(tzinfo=timezone.utc)
    
    # é¢„å¤„ç†ï¼šç§»é™¤æœ«å°¾ç‰¹æ®Šç¬¦å·ï¼ˆç©ºæ ¼ã€â€¢ã€ç‚¹ç­‰ï¼‰
    text = text.strip()
    text = re.sub(r'[â€¢.\s]+$', '', text)  # æ¸…ç†å†—ä½™ç¬¦å·
    now = datetime.now(timezone.utc)
    current_year = now.year

    # å¤„ç†è‹±æ–‡ç›¸å¯¹æ—¶é—´ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if re.search(r"just now|moments ago", text, re.I):
        return now
    if m := re.match(r"^(\d+)\s*m$", text, re.I):  # åˆ†é’Ÿï¼ˆ5mï¼‰
        return now - timedelta(minutes=int(m.group(1)))
    if m := re.match(r"^(\d+)\s*h$", text, re.I):  # å°æ—¶ï¼ˆ3hï¼‰
        return now - timedelta(hours=int(m.group(1)))
    if m := re.match(r"^(\d+)\s*d$", text, re.I):  # å¤©ï¼ˆ2dï¼‰
        return now - timedelta(days=int(m.group(1)))
    if re.search(r"yesterday", text, re.I):  # æ˜¨å¤©
        return now - timedelta(days=1)
    if re.search(r"today", text, re.I):  # ä»Šå¤©
        return now.replace(hour=0, minute=0, second=0, microsecond=0)
    if m := re.match(r"^(\d+)\s*w$", text, re.I):  # å‘¨ï¼ˆ1wï¼‰
        return now - timedelta(weeks=int(m.group(1)))
    if m := re.match(r"^(\d+)\s*mo$", text, re.I):  # æœˆï¼ˆ6moï¼‰
        return now - timedelta(days=int(m.group(1)) * 30)
    if m := re.match(r"^(\d+)\s*y$", text, re.I):  # å¹´ï¼ˆ2yï¼‰
        return now - timedelta(days=int(m.group(1)) * 365)

    # å¤„ç†å…·ä½“æ—¥æœŸæ ¼å¼ï¼ˆæ–°å¢å¯¹ Jun '24 ç­‰æ ¼å¼çš„æ”¯æŒï¼‰
    # 1. æœˆä»½ç¼©å†™ + ' + ä¸¤ä½å¹´ä»½ï¼ˆå¦‚ Jun '24 â†’ 2024å¹´6æœˆ1æ—¥ï¼‰
    if m := re.match(r"^\s*(\w{3})\s*'(\d{2})\s*$", text, re.I):  # å…è®¸å‰åç©ºæ ¼
        month_abbr, year_short = m.groups()
        try:
            # è¡¥å…¨å¹´ä»½ï¼ˆ'24 â†’ 2024ï¼‰ï¼Œé»˜è®¤å½“æœˆ1æ—¥
            full_year = f"20{year_short}"
            return datetime.strptime(
                f"{month_abbr} 01 {full_year}",  # æ ¼å¼ï¼šæœˆä»½ æ—¥ å¹´
                "%b %d %Y"
            ).replace(tzinfo=timezone.utc)
        except ValueError:
            pass  # æ— æ•ˆæœˆä»½æ—¶è·³è¿‡

    # 2. æœˆä»½ç¼©å†™ + æ—¥æœŸï¼ˆå¦‚ Aug 31 â†’ 2024-08-31ï¼‰
    if m := re.match(r"^(\w{3}) (\d{1,2})$", text):
        month_abbr, day = m.groups()
        try:
            return datetime.strptime(
                f"{month_abbr} {day} {current_year}",
                "%b %d %Y"
            ).replace(tzinfo=timezone.utc)
        except ValueError:
            pass

    # 3. æœˆä»½ç¼©å†™ + æ—¥æœŸ, å¹´ä»½ï¼ˆå¦‚ Jun 01, 2024ï¼‰
    if m := re.match(r"^(\w{3}) (\d{1,2}), (\d{4})$", text):
        try:
            return datetime.strptime(text, "%b %d, %Y").replace(tzinfo=timezone.utc)
        except ValueError:
            pass

    # 4. å¹´-æœˆ-æ—¥ï¼ˆå¦‚ 2024-06-01ï¼‰
    if m := re.match(r"^\d{4}-\d{2}-\d{2}$", text):
        try:
            return datetime.strptime(text, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        except ValueError:
            pass

    # æœªåŒ¹é…çš„æ ¼å¼
    print(f"âš ï¸ æœªè¯†åˆ«çš„æ—¶é—´æ ¼å¼: {text}")
    return datetime.min.replace(tzinfo=timezone.utc)


# æ‰“å¼€å¸–å­çš„è¯¦æƒ…é¡µï¼Œæ»šåŠ¨è§¦å‘å…¨éƒ¨æ­£æ–‡åŠ è½½ï¼Œå¹¶è¿”å›æ­£æ–‡çš„çº¯æ–‡æœ¬
def fetch_post_detail_content(post_url: str, storage_state: Optional[str] = None,
                              headless: bool = True, scroll_rounds: int = 3, scroll_pause: float = 1.2,
                              timeout_ms: int = 60000) -> str:
    """æ‰“å¼€å¸–å­è¯¦æƒ…é¡µå¹¶è¿”å›æ¸²æŸ“åçš„ HTMLï¼ˆåŒ…å«æ­£æ–‡ï¼‰"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        ctx_kwargs = {"user_agent": USER_AGENT}
        if storage_state and os.path.exists(storage_state):
            ctx_kwargs["storage_state"] = storage_state
        ctx = browser.new_context(**ctx_kwargs)
        page = ctx.new_page()
        print(f"[fetch_post_detail_content] æ‰“å¼€ {post_url}")
        page.goto(post_url, timeout=timeout_ms)
        page.wait_for_load_state("load")

        # æ»šåŠ¨åŠ è½½æ­£æ–‡ï¼Œæ³¨æ„åŠæ—¶åœæ­¢ï¼Œé¿å¼€ä¸‹é¢çš„è¯„è®º       
        try:
            # ç­‰å¾…æ­£æ–‡åŒºåŸŸåŠ è½½å®Œæˆ
            print("[fetch_post_detail_content] ç­‰å¾…æ­£æ–‡åŒºåŸŸåŠ è½½...")
            page.wait_for_selector("[class*='PostContent'], [class*='PostBody'], article", timeout=5000)
            print("[fetch_post_detail_content] æ­£æ–‡åŒºåŸŸå·²åŠ è½½")
        except PlayTimeoutError:
            print("ä¸‹ä¸‹ä¸‹ï¼[fetch_post_detail_content] è­¦å‘Šï¼šæ­£æ–‡åŒºåŸŸåŠ è½½è¶…æ—¶ï¼Œå¯èƒ½å½±å“å†…å®¹å®Œæ•´æ€§")
        # å¤„ç†å¤šä¸ª"see more"æŒ‰é’®ï¼ˆæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªå¯è§çš„ï¼‰
        see_more_selector = page.locator(
            "[class*='PostContent'], [class*='PostBody'], article"
        ).get_by_text(
            re.compile(r"see more|å±•å¼€|æŸ¥çœ‹æ›´å¤š", re.IGNORECASE),
            exact=False
        )
        
        # æœ€å¤šå°è¯•ç‚¹å‡»5æ¬¡ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        max_clicks = 5
        click_count = 0
        while click_count < max_clicks:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯è§çš„æŒ‰é’®ï¼ˆåªçœ‹ç¬¬ä¸€ä¸ªåŒ¹é…çš„ï¼‰
                if see_more_selector.first.is_visible() and see_more_selector.first.is_enabled():
                    see_more_selector.first.click()  # åªç‚¹å‡»ç¬¬ä¸€ä¸ªå¯è§æŒ‰é’®
                    click_count += 1
                    print(f"[fetch_post_detail_content] å·²ç‚¹å‡»ç¬¬ {click_count} ä¸ªsee moreæŒ‰é’®")
                    time.sleep(1.5)  # ç­‰å¾…å†…å®¹å±•å¼€
                else:
                    break  # æ²¡æœ‰å¯è§æŒ‰é’®äº†ï¼Œé€€å‡ºå¾ªç¯
            except Exception as e:
                print(f"[fetch_post_detail_content] ç‚¹å‡»see moreå¤±è´¥ï¼ˆå·²å°è¯•{click_count}æ¬¡ï¼‰ï¼š{e}")
                break

        # åŸæœ‰æ»šåŠ¨é€»è¾‘ï¼ˆç¡®ä¿å‰©ä½™å†…å®¹åŠ è½½ï¼‰
        last_h = 0
        for _ in range(scroll_rounds):
            try:
                page.evaluate("window.scrollBy(0, document.body.scrollHeight);")
            except Exception:
                pass
            time.sleep(scroll_pause)
            try:
                page.wait_for_load_state("networkidle", timeout=2000)
            except PlayTimeoutError:
                pass
            new_h = page.evaluate("document.body.scrollHeight")
            if new_h == last_h:
                break
            last_h = new_h

        html = page.content()
        browser.close()
    return html


# ä»å¸–å­è¯¦æƒ…é¡µ HTML ä¸­æå–æ­£æ–‡æ–‡æœ¬ã€‚è¿”å›çº¯æ–‡æœ¬ï¼ˆå»é™¤è„šæœ¬ä¸æ ·å¼ï¼‰ã€‚
# ä½¿ç”¨è‹¥å¹²selectoræ¥æ‰¾åˆ°å†…å®¹åŒºåŸŸã€‚
def parse_post_content_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")
    #  ç§»é™¤è¯„è®ºç›¸å…³å…ƒç´ ï¼ˆå‚è€ƒskool_crawler.pyçš„è¯„è®ºé€‰æ‹©å™¨ï¼‰
    comment_selectors = [
        "div[class*='CommentItem']",
        "div[class*='CommentWrapper']",
        "div[class*='Comment']",
        "li[class*='Comment']",
        "section[class*='CommentsSection']",
        "div[class*='CommentList']"
    ]
    for sel in comment_selectors:
        for elem in soup.select(sel):
            elem.decompose()
            
    # ç§»é™¤å…¶ä»–æ— å…³å…ƒç´ 
    for bad in soup(["script", "style", "nav", "footer", "table", "thead", "tbody", "form", "aside"]):
        bad.decompose()

    # å°è¯•å¸¸è§selector
    content_selectors = [
        "div[class*='PostContent']",
        "div[class*='PostBody']",
        "article[class*='Post']",
        "div[class*='PostItemContent']",
        "div[class*='PostDetails']"
    ]
    content_node = None
    for sel in content_selectors:
        node = soup.select_one(sel)
        if node and len(node.get_text(strip=True)) >80:
            content_node = node
            break

    if content_node is None:
        # æ‰¾åˆ°mainä¸‹å«æœ‰æœ€å¤§æ–‡æœ¬é‡çš„æ ‡ç­¾ä½œä¸ºæ›¿ä»£
        main_content = soup.find("main") or soup.body
        if main_content:
            # æ‰¾åˆ°mainä¸­æœ€é•¿çš„æ–‡æœ¬å—
            candidates = []
            for tag in main_content.find_all(['div', 'section', 'article']):
                text = tag.get_text(" ", strip=True)
                if len(text) > 150 and not re.search(r"copyright|terms|policy", text, re.I):
                    candidates.append((len(text), tag))
            if candidates:
                candidates.sort(reverse=True, key=lambda x: x[0])
                content_node = candidates[0][1]  
    # æå–æœ€ç»ˆæ–‡æœ¬
    if content_node:
        text = content_node.get_text(" ", strip=True)
    else:
        text = soup.get_text(" ", strip=True)

    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r"\s{2,}", " ", text).strip()
    return text


# å°†æŠ“å–åˆ°çš„æ–‡æœ¬ä¿å­˜ä¸ºcsvæ ¼å¼æ–‡ä»¶ï¼š
def save_posts_to_csv(posts: List[Dict], path: str):
    ensure_dir_for_file(path)
    df = pd.DataFrame(posts)
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"[save_posts_to_csv] å·²ä¿å­˜ {len(posts)} æ¡åˆ° {path}")


# å°†æŠ“å–åˆ°çš„æ–‡æœ¬ä¿å­˜åˆ°SQLite
def save_posts_to_sqlite(posts: List[Dict], db_path: str = DEFAULT_DB):
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    c.execute("""
    CREATE TABLE IF NOT EXISTS posts (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        url TEXT UNIQUE,
        title TEXT,
        author TEXT,
        time TEXT,
        likes INTEGER,
        comments INTEGER,
        excerpt TEXT,
        content TEXT,
        fetched_at TEXT
    )
    """)
    c.execute("PRAGMA table_info(posts)")
    cols = [row[1] for row in c.fetchall()]
    if "content" not in cols:
        print("[DB] æ£€æµ‹åˆ°æ—§ç‰ˆæ•°æ®åº“ç»“æ„ï¼Œè‡ªåŠ¨æ·»åŠ  content åˆ—...")
        c.execute("ALTER TABLE posts ADD COLUMN content TEXT")
        conn.commit()
    for p in posts:
        now = now_iso()
        c.execute("""
        INSERT OR REPLACE INTO posts (url, title, author, time, likes, comments, excerpt, content, fetched_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (p.get("url"), p.get("title"), p.get("author"), p.get("time"),
              p.get("likes"), p.get("comments"), p.get("excerpt"), p.get("content"), now))
    conn.commit()
    conn.close()
    print(f"[save_posts_to_sqlite] å·²ä¿å­˜ {len(posts)} æ¡åˆ° {db_path}")

# ä½¿ç”¨è±†åŒ…1.6å¯¹æ–‡æœ¬è¿›è¡Œæ€»ç»“
def summarize_with_doubao(posts: List[Dict], doubao_key: str,
                          model: str = "doubao-1-5-pro-32k-250115") -> Dict:
    # å°†å¤šæ¡å¸–å­å†…å®¹åˆå¹¶ï¼Œç„¶åè°ƒç”¨ Doubao API ç”Ÿæˆä¸­æ–‡ç»¼åˆæ‘˜è¦ï¼ˆä¸»é¢˜ + æŠ€æœ¯è¦ç‚¹ï¼‰
    if not posts:
        return {"summary": "", "raw_response": None}
    snippets = []
    for p in posts:
        content = p.get("content") or p.get("excerpt") or ""
        if not content:
            continue
        # keep each content short enough to fit in token limit
        snippets.append(p.get("title", "") + ": " + (content[:1000]))
    merged_text = "\n\n".join(snippets)
    # system_prompt = (
    #     "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æŠ€æœ¯å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚"
    # )
    user_prompt = (
        "ä¸‹é¢æ˜¯æ¥è‡ªä¸€ä¸ªç¤¾ç¾¤çš„å¤šæ¡å¸–å­æ­£æ–‡ï¼ˆå·²å»é‡ã€æŒ‰æ—¶é—´æ’åºï¼‰ã€‚è¯·åŸºäºè¿™äº›å†…å®¹ï¼š\n\n"
        "1) ç»™å‡ºä¸€æ®µä¸­æ–‡çš„ç»¼åˆæ‘˜è¦ï¼Œå¼€å¤´ç”¨â€œè¿™äº›å¸–å­ä¸»è¦è®¨è®ºäº†ï¼šâ€å¹¶ç”¨ä¸€åˆ°ä¸¤æ®µæè¿°ä¸»è¦ä¸»é¢˜ï¼›\n"
        "2) æå–å…³é”®çš„**æŠ€æœ¯è¦ç‚¹**ï¼ˆè¦ç‚¹åŒ–ï¼Œæœ€å¤š 8 æ¡ï¼Œæ¯æ¡ä¸€å¥è¯ï¼‰ï¼›\n"
        "3) ç»™å‡ºæ•´ä½“çš„æƒ…æ„Ÿ/æ€åº¦åˆ¤æ–­ï¼ˆç§¯æ/ä¸­æ€§/è´Ÿé¢ï¼‰ï¼Œå¹¶ç®€è¦è¯´æ˜ä¾æ®ï¼ˆ1-2 å¥ï¼‰ï¼›\n\n"
        "è¯·æ³¨æ„è¾“å‡ºæ ¼å¼ï¼šå…ˆè¾“å‡º <SUMMARY> æ®µï¼ˆçº¯æ–‡æœ¬ï¼‰ï¼Œæ¥ç€è¾“å‡º <KEY_POINTS> åˆ—è¡¨ï¼ˆæ¯æ¡å‰æœ‰ -ï¼‰ï¼Œæœ€åè¾“å‡º <SENTIMENT>ã€‚"
        "\n\nè¾“å…¥æ–‡æœ¬å¦‚ä¸‹ï¼š\n\n" + merged_text
    )

    try:
        # åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯
        client = Ark(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key=doubao_key
        )

        # åˆ›å»ºå¯¹è¯è¡¥å…¨è¯·æ±‚
        print("~~~[summarize_with_doubao] è°ƒç”¨ Doubao API ç”Ÿæˆæ‘˜è¦ï¼ˆå¯èƒ½éœ€è¦å‡ ç§’ï¼‰...")
        completion = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸­æ–‡ç§‘æŠ€æ–‡ç« ç»¼è¿°ä¸“å®¶ã€‚"},
                {"role": "user", "content": user_prompt},
            ],
        )

        summary_text = completion.choices[0].message.content
        return {
            "summary": summary_text.strip(),
            "raw_response": completion.model_dump() if hasattr(completion, "model_dump") else str(completion)
        }
    
    except Exception as e:
        print(f"[summarize_with_doubao] è°ƒç”¨è±†åŒ…å¤±è´¥: {e}")
        # å›é€€åˆ°æŠ½å–å¼æ‘˜è¦
        summary_text = _fallback_extractive_summary(posts)
        return {
            "summary": summary_text,
            "raw_response": {"error": str(e)}
        }


def _fallback_extractive_summary(posts: List[Dict]) -> str:
    # ç®€å•æŠ½å–å¼ï¼šå–æ¯æ¡å‰200å­—æ‹¼æ¥ï¼Œå¹¶åšç®€å•åˆå¹¶ä¸è¦ç‚¹æŠ½å–ï¼ˆè¯é¢‘ï¼‰
    if not posts:
        return ""
    head_texts = [ (p.get("title") or "") + "ï¼š" + ( (p.get("content") or p.get("excerpt") or "")[:200] ) for p in posts[:10] ]
    merged = "\n\n".join(head_texts)
    # ç”Ÿæˆç®€å•è¦ç‚¹ï¼šç»Ÿè®¡é«˜é¢‘è¯ï¼ˆæ’é™¤å¸¸è§åœç”¨è¯ï¼‰
    text_for_freq = re.sub(r"[^\w\u4e00-\u9fff]+", " ", merged.lower())
    words = text_for_freq.split()
    stop = set(["the","and","that","this","with","for","using","use","ai","is","are","to","of","in","a","on","æˆ‘ä»¬","çš„","åœ¨","å’Œ","æ˜¯","ä¸","ä¹Ÿ","å¯ä»¥","é€šè¿‡"])
    freq = {}
    for w in words:
        if len(w) < 2: continue
        if w in stop: continue
        freq[w] = freq.get(w,0)+1
    top = sorted(freq.items(), key=lambda x:-x[1])[:8]
    key_points = [f"- {w} ({c} æ¬¡)" for w,c in top]
    summary = "è¿™äº›å¸–å­ä¸»è¦è®¨è®ºäº†ï¼š" + (merged[:400] + "...") + "\n\nå…³é”®æŠ€æœ¯è¦ç‚¹ï¼š\n" + ("\n".join(key_points) if key_points else "- æ— æ˜æ˜¾é«˜é¢‘æŠ€æœ¯å…³é”®è¯")
    return summary


def run(args):
    html_file = f"skool_{args.group}.html"
    html = fetch_group_html(args.storage_state, args.group, html_file, headless=not args.debug)
    posts_meta = parse_posts_from_html(html, args.group)
    for post in posts_meta:
        post["datetime"] = parse_time(post["time"])
    
    posts_meta_sorted = sorted(
        posts_meta,
        key=lambda x: x["datetime"],
        reverse=True  # æœ€æ–°çš„æ’åœ¨å‰é¢
    )

    posts_meta = posts_meta_sorted[:args.limit]

    posts = []
    for idx, meta in enumerate(posts_meta):
        url = meta.get("url")
        if not url:
            print("é­å•¦ï¼æ­£æ–‡çš„URLæ‰¾ä¸åˆ°äº†å‘œå‘œå‘œ~")
            continue
        try:
            post_html = fetch_post_detail_content(url, storage_state=args.storage_state, headless=not args.debug,
                                                  scroll_rounds=args.post_scrolls, scroll_pause=args.post_scroll_pause)
            content = parse_post_content_from_html(post_html)
            entry = dict(meta)
            entry["content"] = content
            posts.append(entry)
            print(f"[run] ({idx+1}/{len(posts_meta)}) å·²æŠ“å–æ­£æ–‡ï¼Œé•¿åº¦ {len(content)}")
        except Exception as e:
            print(f"[run] æŠ“å–è¯¦æƒ…å¤±è´¥: {url} -> {e}")
        time.sleep(args.delay_between_posts)
    if args.output_csv:
        save_posts_to_csv(posts, args.output_csv)
    if args.output_db:
        save_posts_to_sqlite(posts, args.output_db)
    
    print("[run] å¼€å§‹è°ƒç”¨ Doubao ç”Ÿæˆç»¼åˆæ‘˜è¦ï¼ˆåŸºäºæŠ“å–åˆ°çš„æ‰€æœ‰å¸–å­æ­£æ–‡ï¼‰")
    summary_res = summarize_with_doubao(posts, args.doubao_key, model=args.model)
    summary_text = summary_res.get("summary") if isinstance(summary_res, dict) else str(summary_res)
    raw = summary_res.get("raw_response") if isinstance(summary_res, dict) else None
    summary_obj = {
        "generated_at": now_iso(),
        "group": args.group,
        "limit": args.limit,
        "summary": summary_text,
        "raw_response": raw
    }

    with open(args.summary_out, "w", encoding="utf-8") as f:
        json.dump(summary_obj, f, ensure_ascii=False, indent=2)
    print(f"[run] å·²ä¿å­˜ç»¼åˆæ‘˜è¦åˆ° {args.summary_out}")

    # æ˜“è€å¸ˆè¦æ±‚1ï¼šä¿å­˜ä¸º Markdown æ–‡ä»¶
    from textwrap import dedent
    md_path = args.summary_out.replace(".json", ".md")

    markdown_text = dedent(f"""
    # ğŸ§­ Skool ç¤¾ç¾¤æ‘˜è¦ï¼š{args.group}
    ç”Ÿæˆæ—¶é—´ï¼š{summary_obj['generated_at']}

    ---

    ## æ‘˜è¦
    {summary_obj['summary']}

    ---

    ## åŸå§‹æ•°æ®
    - å¸–å­æ•°é‡ï¼š{len(posts)}
    - æ¥æºæ•°æ®åº“ï¼š{args.output_db or 'æ— '}
    - Doubao æ¨¡å‹ï¼š{args.model}
    """)

    with open(md_path, "w", encoding="utf-8") as f:
        f.write(markdown_text)

    print(f"[run] å·²ä¿å­˜ Markdown æ‘˜è¦åˆ° {md_path}")

    # æ˜“è€å¸ˆè¦æ±‚2ï¼šä¿å­˜ä¸º Word æ–‡ä»¶
    import pypandoc

    docx_path = args.summary_out.replace(".json", ".docx")
    pypandoc.download_pandoc()
    pypandoc.convert_text(markdown_text, "docx", format="md", outputfile=docx_path, extra_args=["--standalone"])
    print(f"[run] å·²ä¿å­˜ Word æ–‡ä»¶åˆ° {docx_path}")
    # æ‰“å°é¢„è§ˆ
    preview = summary_text[:3000] if summary_text else ""
    print("---- è±†åŒ…æ‘˜è¦é¢„è§ˆï¼ˆæœ€å¤š3000å­—ç¬¦ï¼‰ ----")
    print(preview)
    print("---- ç»“æŸ ----")

def build_parser():
    p = argparse.ArgumentParser(description="Skool çˆ¬è™« + Doubao ç»¼åˆæ‘˜è¦")
    p.add_argument("--group", required=True, help="skool group slug, e.g. ai-automation-society")
    p.add_argument("--limit", type=int, default=20, help="è¦æŠ“å–çš„æœ€æ–°å¸–å­æ•°é‡")
    p.add_argument("--storage_state", default="skool_state.json", help="Playwright storage_state.json")
    p.add_argument("--output_csv", default=DEFAULT_CSV, help="è¾“å‡º CSV è·¯å¾„ï¼ˆè®¾ä¸ºç©ºä¸ä¿å­˜ï¼‰")
    p.add_argument("--output_db", default=DEFAULT_DB, help="è¾“å‡º SQLite DB è·¯å¾„ï¼ˆè®¾ä¸ºç©ºä¸ä¿å­˜ï¼‰")
    p.add_argument("--summary_out", default=DEFAULT_SUMMARY_JSON, help="æ‘˜è¦è¾“å‡º JSON è·¯å¾„")
    p.add_argument("--doubao_key", default=DEFAULT_DOUBAO_API_KEY, help="Doubao API Key")
    p.add_argument("--model", default="doubao-1-5-pro-32k-250115", help="Doubao æ¨¡å‹åï¼ˆè‹¥æ— æƒé™ä¼šå›é€€ï¼‰")
    p.add_argument("--max_scrolls", type=int, default=30, help="åˆ—è¡¨é¡µæœ€å¤§æ»šåŠ¨æ¬¡æ•°")
    p.add_argument("--scroll_pause", type=float, default=1.0, help="åˆ—è¡¨é¡µæ»šåŠ¨é—´éš”ç§’")
    p.add_argument("--post_scrolls", type=int, default=8, help="è¯¦æƒ…é¡µæ»šåŠ¨æ¬¡æ•°")
    p.add_argument("--post_scroll_pause", type=float, default=1.2, help="è¯¦æƒ…é¡µæ»šåŠ¨é—´éš”ç§’")
    p.add_argument("--delay_between_posts", dest="delay_between_posts", type=float, default=1.2, help="æŠ“å–è¯¦æƒ…é—´å»¶è¿Ÿ")
    p.add_argument("--debug", action="store_true", help="æ˜¾ç¤ºæµè§ˆå™¨ï¼ˆè°ƒè¯•ï¼‰")
    return p

if __name__ == "__main__":
    parser = build_parser()
    args = parser.parse_args()

    if args.output_csv == "":
        args.output_csv = None
    if args.output_db == "":
        args.output_db = None

    run(args)
    
