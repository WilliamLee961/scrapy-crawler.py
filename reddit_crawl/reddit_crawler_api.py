# è¯¥pythonæ–‡ä»¶çš„ä»»åŠ¡æ˜¯å®Œæˆçˆ¬è™«ç­–ç•¥çš„apiæ¥å£
import os
import time
import json
from fastapi.responses import JSONResponse  # å¯¼å…¥JSONResponse
import traceback
import threading
from datetime import datetime
from typing import List, Dict, Optional, Any
from fastapi import FastAPI, HTTPException, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel  # æ•°æ®æ ¡éªŒ
import uvicorn
# å¯¼å…¥ä½ çš„ç°æœ‰æ¨¡å—
from reddit_crawl.reddit_crawler import RedditCrawler # åŸºç¡€çˆ¬è™«æ¨¡å—
from reddit_crawl.notification import (
    load_processed_posts, 
    save_processed_posts,
    log_post_info
)
from reddit_crawl.anti_crawl_core import ip_pool, smart_strategy # åçˆ¬æ ¸å¿ƒæ¨¡å—,è°ƒç”¨ç±»ï¼ˆåŒ…å«ç±»ä¸­å‡½æ•°ï¼‰

# ---------------------- 1. å…¨å±€é…ç½®ä¸åˆå§‹åŒ– ----------------------
# APIæœåŠ¡é…ç½®
API_CONFIG = {
    "host": "127.0.0.1",        # å…è®¸å¤–éƒ¨è®¿é—®
    "port": 8000,             # APIç«¯å£
    "api_key": "RedditCrawler_2024",  # APIé‰´æƒå¯†é’¥ï¼ˆé¿å…æœªæˆæƒè®¿é—®ï¼‰
    "log_file_path": "reddit_posts.log",  # æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆä¸notification.pyä¸€è‡´ï¼‰
    "max_concurrent": 100     # æ”¯æŒæœ€å¤§å¹¶å‘ä¼šè¯ï¼ˆæ»¡è¶³ç”²æ–¹æŒ‡æ ‡ï¼‰
}

# åˆå§‹åŒ–FastApiåº”ç”¨
app = FastAPI(
    title = "Redditçˆ¬è™«å¯¹å¤–APIæœåŠ¡",
    description="åŠŸèƒ½ï¼šå®ç°çˆ¬è™«çŠ¶æ€æŸ¥è¯¢ã€æ—¥å¿—è¯»å–ã€åçˆ¬ç­–ç•¥é…ç½®ï¼ˆIPæ± /æ™ºèƒ½ç­–ç•¥ï¼‰",
    version= "1.0.0", 
     default_response_class=JSONResponse,
    # 2. æ­£ç¡®é…ç½®responsesï¼šé”®ä¸ºçŠ¶æ€ç ï¼ˆintï¼‰ï¼Œå€¼ä¸ºè¯¥çŠ¶æ€ç çš„å“åº”é…ç½®
    responses={
        200: {  # é’ˆå¯¹200çŠ¶æ€ç çš„å“åº”é…ç½®ï¼ˆæœ€å¸¸ç”¨ï¼‰
            "description": "è¯·æ±‚æˆåŠŸ",
            "content": {
                "application/json": {  # MIMEç±»å‹æ”¾åœ¨contentä¸‹
                    "charset": "utf-8",  # æ˜¾å¼æŒ‡å®šUTF-8ç¼–ç ï¼Œè§£å†³ä¸­æ–‡ä¹±ç 
                    "example": {  # å¯é€‰ï¼šæ·»åŠ ç¤ºä¾‹ï¼Œæ–¹ä¾¿è°ƒè¯•
                        "code": 200,
                        "message": "æ“ä½œæˆåŠŸ",
                        "data": {}
                    }
                }
            }
        },
        400: {  # å¯é€‰ï¼šé’ˆå¯¹400é”™è¯¯çŠ¶æ€ç çš„é…ç½®ï¼ˆç¤ºä¾‹ï¼‰
            "description": "è¯·æ±‚å‚æ•°é”™è¯¯",
            "content": {
                "application/json": {
                    "charset": "utf-8",
                    "example": {
                        "detail": "å‚æ•°é”™è¯¯ï¼Œéœ€è‡³å°‘ä¼ å…¥ä¸€ä¸ªå­—æ®µ"
                    }
                }
            }
        }
    }
)

# è§£å†³è·¨åŸŸé—®é¢˜ï¼ˆå…è®¸å‰ç«¯/å…¶ä»–æœåŠ¡è°ƒç”¨APIï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # ç”Ÿäº§ç¯å¢ƒéœ€æ›¿æ¢ä¸ºå…·ä½“åŸŸåï¼ˆå¦‚ ["https://your-frontend.com"]ï¼‰
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------- 2. çˆ¬è™«çŠ¶æ€ç®¡ç†ï¼ˆä¸APIå¼ºç›¸å…³ï¼Œæ”¾åœ¨APIæ–‡ä»¶ä¸­ï¼‰ ----------------------
class CrawlerState:
    """çˆ¬è™«è¿è¡ŒçŠ¶æ€ç®¡ç†ï¼ˆç›‘æ§å¹¶å‘ã€å»¶è¿Ÿã€æ¨é€å“åº”æ—¶é—´ï¼Œä¾èµ–åçˆ¬æ¨¡å—ï¼‰"""
    def __init__(self):
        self.state = {
            "is_running": False, # çˆ¬è™«è¿è¡ŒçŠ¶æ€
            "current_concurrent": 0, # å½“å‰å¹¶å‘ä¼šè¯æ•°
            "recent_crawl_delays": [], # æœ€è¿‘10æ¬¡é‡‡é›†å»¶è¿Ÿï¼ˆç§’ï¼‰
            "last_push_response_time": None, # æœ€åä¸€æ¬¡æ¨é€å“åº”æ—¶é—´(ä»ä¸Šæ¬¡æ¨é€å“åº”åˆ°è¿™æ¬¡æ¨é€å“åº”)
            "total_crawled_posts": 0, # ç´¯è®¡é‡‡é›†å¸–å­
            "total_pushed_posts": 0 # ç´¯è®¡æ¨é€å¸–å­
        }
        self.lock = threading.Lock()
        self.crawler_thread: Optional[threading.Thread] = None # çˆ¬è™«åå°çº¿ç¨‹
        self.crawled_results = []
        self.results_lock = threading.Lock() # ä¿æŠ¤ç»“æœåˆ—è¡¨çš„çº¿ç¨‹é”

    def update_crawl_delay(self, delay: float):
        """æ›´æ–°é‡‡é›†å»¶è¿Ÿï¼ˆä¿ç•™æœ€è¿‘10æ¬¡æ•°æ®ï¼Œç”¨äºè®¡ç®—å¹³å‡å»¶è¿Ÿï¼‰"""
        with self.lock:
            self.state["recent_crawl_delays"].append(round(delay, 2)) # ä¿ç•™ä¸¤ä½å°æ•°
            if len(self.state["recent_crawl_delays"])> 10:
                self.state["recent_crawl_delays"].pop(0) # æŠŠæœ€æ—§çš„å¼¹å‡ºï¼Œåªç®—æœ€è¿‘10æ¬¡
    
    def update_push_response_time(self, response_time: float):
        """æ›´æ–°æ¨é€å“åº”æ—¶é—´ï¼ˆè®°å½•æœ€åä¸€æ¬¡ï¼Œç”¨äºéªŒè¯â‰¤2åˆ†é’ŸæŒ‡æ ‡ï¼‰ åº”è¯¥æ˜¯ä»å¸–å­å‘å‡ºåˆ°æˆ‘ä»¬åº”ç”¨æ¨é€åˆ°ä¸ªäººé‚®ç®±"""
        with self.lock:
            self.state["last_push_response_time"] = round(response_time, 2)
            self.state["total_pushed_posts"] +=1
    
    def increment_concurrent(self) -> bool:
        """å¢åŠ å¹¶å‘ä¼šè¯æ•°ï¼ˆä¸è¶…è¿‡åçˆ¬ç­–ç•¥çš„å¹¶å‘é™åˆ¶ï¼‰"""
        with self.lock:
            strategy = smart_strategy.get_current_strategy()
            if self.state["current_concurrent"] < strategy["concurrent_limit"]:
                self.state["current_concurrent"] +=1
                return True
            print(f" å¹¶å‘æ•°å·²è¾¾ä¸Šé™ï¼ˆ{strategy['concurrent_limit']}ï¼‰ï¼Œæ— æ³•æ–°å¢ä¼šè¯")
            return False
    
    def decrement_concurrent(self):
        """å‡å°‘å¹¶å‘ä¼šè¯æ•°ï¼ˆé‡‡é›†å®Œæˆ/å¤±è´¥æ—¶è°ƒç”¨ï¼‰"""
        with self.lock:
            if self.state["current_concurrent"] >0:
                self.state["current_concurrent"]-=1
                # å½“å‡å°‘åå˜ä¸º0æ—¶ï¼Œæ‰“å°æé†’
                if self.state["current_concurrent"] == 0:
                    print(" å½“å‰å¹¶å‘ä¼šè¯æ•°å·²å‡è‡³0")

    def test_single_crawl(self) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡çˆ¬å–ï¼Œè¿”å›è¯¦ç»†ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼Œä¸ä¾èµ–åå°çº¿ç¨‹ï¼‰"""
        try:
            print("\n" + "="*60)
            print(" æ‰‹åŠ¨è§¦å‘å•æ¬¡çˆ¬å–æµ‹è¯•")
            print("="*60)

            # 1.è·å–IPå’Œç­–ç•¥
            ip_pool_status = ip_pool.get_pool_status()["statistics"]
            strategy = smart_strategy.get_current_strategy()

            # 2. è·å–IP
            if ip_pool_status["valid_ip_count"] > 0:
                selected_ip = ip_pool.get_random_valid_ip()
                if selected_ip:
                    PROXY_HOST = selected_ip["ip"].split(":")[0]
                    PROXY_PORT = int(selected_ip["ip"].split(":")[1])
                    print(f"æµ‹è¯•ç”¨IPï¼š{selected_ip['ip']}ï¼ˆåè®®ï¼š{selected_ip['protocol']}ï¼‰")
            else:
                print("æµ‹è¯•ï¼šIPæ± æ— æœ‰æ•ˆIPï¼Œç”¨æ— ä»£ç†")
                
            # 3. æ‰§è¡Œçˆ¬å–
            print(f"æµ‹è¯•çˆ¬å–å‚æ•°ï¼šå­ç‰ˆå—={strategy['target_subreddit']}ï¼Œ limit={strategy['max_posts_per_crawl']}")
            crawler = RedditCrawler(proxy_host=PROXY_HOST, proxy_port=PROXY_PORT)
            new_posts = crawler.get_new_posts(
                subreddit_name=strategy["target_subreddit"],
                limit=strategy["max_posts_per_crawl"],
                max_comments=3
            )

            # 4. å­˜å‚¨ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            result = {
                "success": True,
                "crawled_count": len(new_posts),
                "posts": new_posts,
                "message": f"æµ‹è¯•æ‰‹åŠ¨çˆ¬å–æˆåŠŸï¼Œè·å– {len(new_posts)} æ¡å¸–å­"
            }

            if new_posts:
                self.add_crawled_result(result)
                result["message"] += "ï¼ˆå·²å­˜å…¥ç»“æœå®¹å™¨ï¼‰"
            print(f" æµ‹è¯•ä¸­æ‰‹åŠ¨çˆ¬å–ç»“æœï¼š{result['message']}")
            return result
        
        except Exception as e:
            error_message = f"æµ‹è¯•æ‰‹åŠ¨çˆ¬å–å¤±è´¥: str{e}"
            return {
                "success": False,
                "crawled_count": 0,
                "posts": [],
                "message": f"æµ‹è¯•æ‰‹åŠ¨çˆ¬å–å¤±è´¥ï¼Œè·å–0æ¡å¸–å­ï¼ "
            }

    def start_crawler(self) -> bool:
        """å¯åŠ¨çˆ¬è™«ï¼ˆåå°çº¿ç¨‹è¿è¡Œï¼Œé¿å…é˜»å¡APIï¼‰"""
        with self.lock:
            if self.state["is_running"]:
                print(" çˆ¬è™«å·²ç»åœ¨è¿è¡Œï¼Œ æ— éœ€é‡å¤å¯åŠ¨ï¼")
                return False
            self.crawler_thread = threading.Thread(
                target= self._crawler_main_loop,
                daemon=True
            )
            self.crawler_thread.start()
            self.state["is_running"] = True
            print(" çˆ¬è™«å·²å¯åŠ¨ï¼ˆåå°çº¿ç¨‹ï¼‰")
            return True

    def add_crawled_result(self, posts):
        with self.results_lock:
            self.crawled_results.extend(posts) #  åˆå¹¶çˆ¬å–çš„å¸–å­
            if len(self.crawled_results) > 1000:
                self.crawled_results = self.crawled_results[-1000:]  # åªä¿ç•™æœ€è¿‘1000æ¡

    def get_crawled_posts(self, limit=100):
        with self.results_lock:
            # è¿”å›å‰¯æœ¬ï¼Œé¿å…å¤–éƒ¨ä¿®æ”¹
            return self.crawled_results[-limit:] # åªä¿ç•™æœ€è¿‘1000æ¡

    def stop_crawler(self) -> bool:
        """åœæ­¢çˆ¬è™«ï¼ˆå®‰å…¨é€€å‡ºï¼Œä¿å­˜å·²å¤„ç†è®°å½•ï¼‰"""
        with self.lock:
            if not self.state["is_running"]:
                print(" çˆ¬è™«å·²åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ")
                return False
            self.state["is_running"] = False
            #  ç­‰å¾…çº¿ç¨‹é€€å‡ºï¼ˆæœ€å¤š5ç§’ï¼Œé¿å…å¼ºåˆ¶ç»ˆæ­¢å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼‰
            if self.crawler_thread and self.crawler_thread.is_alive():
                self.crawler_thread.join(timeout=5)
                if self.crawler_thread.is_alive():
                    print("çˆ¬è™«çº¿ç¨‹æœªåŠæ—¶é€€å‡ºï¼Œå¯èƒ½å­˜åœ¨æœªå®Œæˆä»»åŠ¡")  
            # é€€å‡ºå‰ä¿å­˜å·²å¤„ç†è®°å½•
            save_processed_posts(load_processed_posts())
            print(" çˆ¬è™«å·²åœæ­¢ï¼Œå·²ä¿å­˜å¤„ç†è®°å½•")
            return True
        
    def _crawler_main_loop(self):
        """çˆ¬è™«ä¸»å¾ªç¯ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼šè°ƒç”¨åçˆ¬ç­–ç•¥+åŸæœ‰çˆ¬è™«+é€šçŸ¥ï¼‰"""
        print(f"[åå°çˆ¬è™«] çº¿ç¨‹å·²å¯åŠ¨ï¼Œè¿›å…¥å¾ªç¯ï¼ˆis_running: {self.state['is_running']}ï¼‰")  # æ–°å¢ï¼šç¡®è®¤çº¿ç¨‹å¯åŠ¨
        while self.state["is_running"]:
            print(f"\n[åå°çˆ¬è™«] è¿›å…¥å¾ªç¯è¿­ä»£ï¼ˆå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%H:%M:%S')}ï¼‰")  # æ–°å¢ï¼šç¡®è®¤å¾ªç¯æ‰§è¡Œ
            try:
                print(f"[åå°çˆ¬è™«] å¼€å§‹æ£€æŸ¥IPæ± çŠ¶æ€")  # æ–°å¢ï¼šå®šä½åˆ°IPæ± ç¯èŠ‚
                # PROXY_HOST = None
                # PROXY_PORT = None
                # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨åˆ‡æ¢IPï¼ˆè°ƒç”¨åçˆ¬æ¨¡å—çš„æ™ºèƒ½ç­–ç•¥ï¼‰
                ip_pool_status = ip_pool.get_pool_status()["statistics"]
                if ip_pool_status["valid_ip_count"] > 0:
                    print(f"[åå°çˆ¬è™«] IPæ± æœ‰ {ip_pool_status['valid_ip_count']} ä¸ªæœ‰æ•ˆIP")  # æ–°å¢ï¼šç¡®è®¤IPæ± æœ‰æ•ˆ
                    if smart_strategy.need_auto_switch_ip():
                        print(f"ğŸ”„ éœ€è¦åˆ‡æ¢IPï¼Œä»IPæ± è·å–æ–°ä»£ç†")
                        selected_ip = ip_pool.get_random_valid_ip()
                    else:
                        # ä¸éœ€è¦åˆ‡æ¢æ—¶ï¼Œè·å–å½“å‰å·²é€‰ä¸­çš„IPï¼ˆå¦‚æœæ²¡æœ‰åˆ™éšæœºå–ä¸€ä¸ªï¼‰
                        print("ä¸éœ€è¦åˆ‡æ¢IP!ç›´æ¥å¼€å§‹çˆ¬å–ä»»åŠ¡ï¼") # å¢åŠ è¾“å‡ºæ¥ç›‘æ§ä»£ç è¿è¡Œæƒ…å†µ
                        selected_ip = ip_pool.get_current_ip() or ip_pool.get_random_valid_ip()
                    
                    # åªè¦è·å–åˆ°IPï¼Œå°±æ›´æ–°ä»£ç†å‚æ•°
                    if selected_ip:
                        # æ–°å¢è°ƒè¯•æ‰“å°ï¼šæŸ¥çœ‹ selected_ip çš„ç±»å‹å’Œå€¼
                        print(f"è°ƒè¯•ï¼šselected_ip ç±»å‹ = {type(selected_ip)}ï¼Œå€¼ = {selected_ip}")
                        # æ›´æ–°çˆ¬è™«çš„ä»£ç†é…ç½®ï¼ˆä¿®æ”¹RedditCrawlerçš„å…¨å±€ä»£ç†å‚æ•°ï¼‰
                        ip_parts = selected_ip["ip"].split(":")
                        PROXY_HOST = ip_parts[0]  # å±€éƒ¨å˜é‡ï¼Œä»…åœ¨å½“å‰ifå—å†…æœ‰æ•ˆ
                        PROXY_PORT = int(ip_parts[1])
                        print(f"å·²è·å–ä»£ç†IPï¼š{PROXY_HOST}:{PROXY_PORT}ï¼ˆåè®®ï¼š{selected_ip['protocol']}ï¼‰")
                else:
                    print(f"[åå°çˆ¬è™«] IPæ± æ— æœ‰æ•ˆIPï¼Œå°è¯•æ— ä»£ç†çˆ¬å–")  # æ–°å¢ï¼šå®šä½IPæ± é—®é¢˜
                    print("IPæ± æ— æœ‰æ•ˆIPï¼Œå°†å°è¯•æ— ä»£ç†è¿æ¥ï¼ˆå¯èƒ½å¤±è´¥ï¼‰")
                
                # 2.æ£€æŸ¥å¹¶å‘é™åˆ¶ï¼ˆè°ƒç”¨åçˆ¬æ¨¡å—çš„ç­–ç•¥ï¼‰
                print(f"[åå°çˆ¬è™«] æ£€æŸ¥å¹¶å‘é™åˆ¶ï¼ˆå½“å‰ï¼š{self.state['current_concurrent']}ï¼Œä¸Šé™ï¼š{smart_strategy.get_current_strategy()['concurrent_limit']}ï¼‰")  # æ–°å¢ï¼šç¡®è®¤å¹¶å‘æ•°
                if not self.increment_concurrent():
                    print(f"[åå°çˆ¬è™«] å¹¶å‘æ•°è¾¾ä¸Šé™ï¼Œç­‰å¾…1ç§’")  # æ–°å¢ï¼šå®šä½å¹¶å‘æ‹¦æˆª
                    strategy = smart_strategy.get_current_strategy()
                    print(f"å¹¶å‘æ•°è¾¾ä¸Šé™ï¼ˆå½“å‰ï¼š{self.state['current_concurrent']}ï¼Œä¸Šé™ï¼š{strategy['concurrent_limit']}ï¼‰ï¼Œç­‰å¾…1ç§’")
                    time.sleep(1)
                    continue
                print(f"å¹¶å‘æ•°å·²å¢åŠ ï¼Œå½“å‰ï¼š{self.state['current_concurrent']}")
                print(f"[åå°çˆ¬è™«] å¹¶å‘æ•°å·²å¢åŠ ï¼Œå¼€å§‹çˆ¬å–")  # æ–°å¢ï¼šç¡®è®¤è¿›å…¥çˆ¬å–
                # 3.æ‰§è¡Œé‡‡é›†ï¼ˆè°ƒç”¨åŸæœ‰çˆ¬è™«æ¨¡å—ï¼Œè®°å½•å»¶è¿Ÿï¼‰
                strategy = smart_strategy.get_current_strategy()
                print(f"[åå°çˆ¬è™«] å¼€å§‹æ‰§è¡Œçˆ¬å–ï¼ˆå­ç‰ˆå—ï¼š{smart_strategy.get_current_strategy()['target_subreddit']}ï¼‰")  # æ–°å¢ï¼šç¡®è®¤çˆ¬å–ç›®æ ‡
                start_time = time.time()

                # åˆå§‹åŒ–çˆ¬è™«ï¼ˆä½¿ç”¨å½“å‰ä»£ç†å’Œç­–ç•¥å‡½æ•°ï¼‰
                crawler = RedditCrawler(
                    proxy_host=PROXY_HOST,
                    proxy_port=PROXY_PORT
                )
                print("çˆ¬è™«ä¸»å¾ªç¯ä¸­å¼€å§‹çˆ¬è™«ï¼")
                new_posts = crawler.get_new_posts(
                    subreddit_name=strategy["target_subreddit"],
                    limit = strategy["max_posts_per_crawl"],
                    max_comments=3 # å›ºå®šå–å‰3æ¡è¯„è®ºï¼ˆå¯åç»­åŠ å…¥ç­–ç•¥å‚æ•°ï¼‰
                )

                if new_posts:
                    self.add_crawled_result(new_posts)
                    print(f"å·²å°† {len(new_posts)} æ¡å¸–å­å­˜å…¥ç»“æœå®¹å™¨")

                # è®°å½•é‡‡é›†å»¶è¿Ÿ
                crawl_delay = time.time() - start_time
                self.update_crawl_delay(crawl_delay)
                self.state["total_crawled_posts"] += len(new_posts)
                print(f"é‡‡é›†å®Œæˆï¼š{len(new_posts)} æ¡å¸–å­ï¼Œå»¶è¿Ÿï¼š{crawl_delay:.2f}ç§’")

                # 4 . æ‰§è¡Œæ¨é€ï¼ˆè°ƒç”¨åŸæœ‰é€šçŸ¥æ¨¡å—ï¼Œè®°å½•å“åº”æ—¶é—´ï¼‰
                if new_posts:
                    push_start = time.time()
                    processed_ids = load_processed_posts()
                    # ç­›é€‰ä¸ºå¤„ç†çš„æ–°å¸–å­ï¼ˆå»é‡ï¼‰
                    new_undetected = [p for p in new_posts if p["id"] not in processed_ids]

                    if new_undetected:
                        # æŒ‰å‘å¸ƒæ—¶é—´æ­£åºè®°å½•æ—¥å¿—ï¼ˆç¬¦åˆé˜…è¯»ä¹ æƒ¯ï¼‰
                        new_undetected_sorted = sorted(
                            new_undetected,
                            key=lambda x: x["created_utc"]
                        )
                        for post in new_undetected_sorted:
                            log_post_info(post)
                
                        # ä¿å­˜æ›´æ–°åçš„å·²å¤„ç†è®°å½•
                        save_processed_posts(processed_ids + [p["id"] for p in new_undetected]) 
                        # processed_ids æ˜¯åˆ—è¡¨ï¼ˆä» load_processed_posts è·å–ï¼‰ï¼Œè€Œ p["id"] for p in new_undetected æ˜¯ç”Ÿæˆå™¨
                        # ï¼ˆç±»å‹ä¸º generatorï¼‰ï¼Œä¸¤è€…æ— æ³•ç”¨ + æ‹¼æ¥ï¼ˆåˆ—è¡¨åªèƒ½ä¸åˆ—è¡¨æ‹¼æ¥ï¼‰!ï¼›
                        
                        # è®°å½•æ¨é€å“åº”æ—¶é—´
                        push_delay = time.time() - push_start
                        self.update_push_response_time(push_delay)
                        print(f"æ¨é€å®Œæˆï¼š{len(new_undetected)} æ¡æ–°å¸–å­ï¼Œå“åº”æ—¶é—´ï¼š{push_delay:.2f}ç§’")

                # 5. é‡Šæ”¾å¹¶å‘æ•°ï¼Œ æŒ‰ç­–ç•¥ä¼‘çœ ï¼ˆè°ƒç”¨åçˆ¬æ¨¡å—çš„é—´éš”å‚æ•°ï¼‰
                self.decrement_concurrent() # æ¯ä¸ªä»»åŠ¡çš„å¯åŠ¨ï¼ˆ+1ï¼‰å¿…ç„¶å¯¹åº”ä¸€ä¸ªä»»åŠ¡çš„ç»“æŸï¼ˆ-1ï¼‰ï¼Œç¡®ä¿ current_concurrent å§‹ç»ˆå‡†ç¡®åæ˜  â€œå½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡æ•°é‡â€
                print(f"æŒ‰ç­–ç•¥ä¼‘çœ  {strategy['crawl_interval']} ç§’...\n")
                time.sleep(strategy["crawl_interval"])

            except Exception as e:
                # é‡‡é›†/æ¨é€å¤±è´¥ï¼š é‡Šæ”¾å¹¶å‘æ•°ï¼Œ æ‰“å°é”™è¯¯æ—¥å¿—
                print(f"[åå°çˆ¬è™«] å¾ªç¯å†…å¼‚å¸¸ï¼š{str(e)}")
                self.decrement_concurrent()
                error_msg = f"çˆ¬è™«å¾ªç¯å‡ºé”™ï¼š{str(e)[:100]}"
                print(error_msg)
                traceback.print_exc()
                print(f" å‡ºé”™åä¼‘çœ 5ç§’ï¼Œé‡è¯•...\n")
                time.sleep(5) # å‡ºé”™åä¼‘çœ 5ç§’å†é‡è¯•ï¼Œé¿å…é¢‘ç¹æŠ¥é”™

    def get_current_state(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«å®æ—¶çŠ¶æ€ï¼ˆå«æŒ‡æ ‡è¾¾æ ‡æƒ…å†µï¼šé‡‡é›†å»¶è¿Ÿâ‰¤3ç§’ã€æ¨é€â‰¤2åˆ†é’Ÿï¼‰"""
        with self.lock:
            delays = self.state["recent_crawl_delays"] # æ³¨æ„è¿™ä¸ªåªæœ‰è¿‘10æ¬¡
            avg_delay = round(sum(delays)/ len(delays), 2) if delays else 0.0

            return {
                "basic_status": {
                    "is_running": self.state["is_running"],
                    "current_concurrent": self.state["current_concurrent"],
                    "total_crawled_posts": self.state["total_crawled_posts"],
                    "total_pushed_posts": self.state["total_pushed_posts"]
                },

                "performance_metrics": {
                    "recent_crawl_delays": self.state["recent_crawl_delays"],
                    "avg_crawl_delay": avg_delay,
                    "last_push_response_time": self.state["last_push_response_time"],
                    # ç”²æ–¹æŒ‡æ ‡è¾¾æ ‡åˆ¤æ–­---é‡‡é›†å»¶è¿Ÿå°äº3ç§’ï¼Œ æ¨é€å»¶è¿Ÿå°äº2åˆ†é’Ÿ
                    "is_crawl_delay_qualified": avg_delay <= 3,
                    "is_push_qualified": (
                        self.state["last_push_response_time"] is None #è€ƒè™‘ç¬¬ä¸€æ¬¡æ¨é€çš„æƒ…å†µ
                        or self.state["last_push_response_time"] <= 120
                    )
                }
            }

# åˆå§‹åŒ–çˆ¬è™«çŠ¶æ€ç®¡ç†ï¼ˆå…¨å±€å”¯ä¸€ï¼‰
crawler_state = CrawlerState()


# ---------------------- 3. APIé‰´æƒï¼ˆé¿å…æœªæˆæƒè®¿é—®ï¼‰ ----------------------
def verify_api_key(api_key: str = Query(..., description="APIè®¿é—®å¯†é’¥")):
    """APIå¯†é’¥é‰´æƒï¼ˆæ‰€æœ‰æ¥å£éœ€æºå¸¦è¯¥å‚æ•°ï¼Œä¾èµ–æ³¨å…¥å®ç°ï¼‰"""
    if api_key != API_CONFIG["api_key"]:
        raise HTTPException(status_code=401, detail="æ— æ•ˆçš„APIå¯†é’¥,è¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
    return api_key


# ---------------------- 4. APIè¯·æ±‚/å“åº”æ¨¡å‹ï¼ˆPydanticå®šä¹‰ï¼‰ ----------------------
class IPAddRequest(BaseModel):
    """æ·»åŠ IPåˆ°IPæ± çš„è¯·æ±‚æ¨¡å‹ï¼ˆå‚æ•°æ ¡éªŒï¼‰"""
    ip: str # æ ¼å¼"ip:port"ï¼ˆå¦‚ "127.0.0.1:7891"ï¼‰
    protocol: Optional[str] = "http" # å¯é€‰ï¼š "sock5" or "http"

class StrategyUpdateRequest(BaseModel):
    """æ›´æ–°æ™ºèƒ½ç­–ç•¥çš„è¯·æ±‚æ¨¡å‹ï¼ˆä»…å…è®¸ä¿®æ”¹é¢„è®¾å­—æ®µï¼‰"""
    # Noneè¡¨ç¤º"æ— å€¼"æˆ–"æœªè®¾ç½®"ï¼Œæ˜¯ä¸€ä¸ªæ˜ç¡®çš„ç©ºå€¼çŠ¶æ€ã€‚
    concurrent_limit: Optional[int] = None
    crawl_interval: Optional[int] = None
    ip_switch_interval: Optional[int] = None
    retry_count: Optional[int] = None
    target_subreddit: Optional[str] = None
    max_posts_per_crawl: Optional[int] = None


# ---------------------- 5. APIç«¯ç‚¹å®ç°ï¼ˆæŒ‰åŠŸèƒ½åˆ†ç±»ï¼‰ ----------------------
@app.get("/api/crawler/state", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"]) # getæ˜¯è·å–æ•°æ®ï¼ˆæŸ¥è¯¢ï¼‰
def get_crawler_state_api(
    api_key: str = Depends(verify_api_key)
    ) -> Dict[str, Any]:
    """
    è·å–çˆ¬è™«å®æ—¶çŠ¶æ€
    - åŸºç¡€çŠ¶æ€ï¼šè¿è¡ŒçŠ¶æ€ã€å¹¶å‘æ•°ã€ç´¯è®¡é‡‡é›†/æ¨é€æ•°
    - æ€§èƒ½æŒ‡æ ‡ï¼šå¹³å‡é‡‡é›†å»¶è¿Ÿã€æ¨é€å“åº”æ—¶é—´ã€æŒ‡æ ‡è¾¾æ ‡æƒ…å†µ
    """
    return {
        "code": 200,
        "message": "success",
        "data": crawler_state.get_current_state()
    }

@app.post("/api/crawler/start", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"]) # postæ˜¯æäº¤æ•°æ®
def start_crawler_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """å¯åŠ¨çˆ¬è™«ï¼ˆåå°çº¿ç¨‹è¿è¡Œï¼Œä¸é˜»å¡APIï¼‰"""
    success = crawler_state.start_crawler()
    if success:
        return {
            "code": 200,
            "message": "çˆ¬è™«å·²å¯åŠ¨",
            "data": {}
        }
    raise HTTPException(status_code=400, detail="çˆ¬è™«å·²åœ¨è¿›è¡Œä¸­ï¼Œæ— éœ€é‡å¤å¯åŠ¨")

@app.get("/api/crawler/results", tags=["1.çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def get_crawler_results(
    limit: int = Query(100, description="è¿”å›ç»“æœçš„æœ€å¤§æ•°é‡"),
    api_key = Depends(verify_api_key)
) -> Dict[str, Any]:
    """è·å–å·²çˆ¬å–çš„å¸–å­ç»“æœï¼ˆä»ç»“æœå®¹å™¨ä¸­è¯»å–ï¼‰"""
    results = crawler_state.get_crawled_posts(limit)
    return {
        "code": 200,
        "message": f"æˆåŠŸè·å– {len(results)} æ¡ç»“æœ",
        "data": {
            "posts": results,
            "total": len(results)
        }
    }

@app.post("/api/crawler/test-crawl", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def test_crawl_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """
    æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡çˆ¬å–ï¼ˆç”¨äºæµ‹è¯•ï¼ä¸ä¾èµ–åå°çº¿ç¨‹ï¼‰
    - ç›´æ¥æ‰§è¡Œä¸€æ¬¡å®Œæ•´çˆ¬å–æµç¨‹ï¼Œè¿”å›çˆ¬å–ç»“æœ
    - æ–¹ä¾¿å¿«é€ŸéªŒè¯ï¼šæ˜¯å¦èƒ½çˆ¬å–åˆ°æ•°æ®ã€ç»“æœæ˜¯å¦å­˜å…¥å®¹å™¨
    """
    test_result = crawler_state.test_single_crawl()
    if test_result["success"]:
        return {
            "code": 200,
            "message": test_result["message"],
            "data": {
                "crawled_count": test_result["crawled_count"],
                "posts": test_result["posts"],
                "container_total": len(crawler_state.crawled_results)  # å®¹å™¨å½“å‰æ€»æ¡æ•°
            }
        }
    raise HTTPException(status_code=500, detail=test_result["message"])


@app.post("/api/crawler/stop", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def stop_crawler_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """åœæ­¢çˆ¬è™«ï¼ˆå®‰å…¨é€€å‡ºï¼Œè‡ªåŠ¨ä¿å­˜å·²å¤„ç†è®°å½•ï¼‰"""
    success = crawler_state.stop_crawler()
    if success:
        return {
            "code": 200,
            "message": "çˆ¬è™«å·²åœæ­¢ï¼Œ å·²ä¿å­˜å¤„ç†è®°å½•",
            "data": {}
        }
    raise HTTPException(status_code=400, detail="çˆ¬è™«å·²åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ")

@app.get("/api/log", tags =["2. æ—¥å¿—ç®¡ç†"])
def get_logs_api(
    start_time: Optional[datetime] = Query(None, description= "æ—¥å¿—å¼€å§‹æ—¶é—´ï¼ˆå¦‚ 2025-09-19T12:00:00ï¼‰"),
    end_time: Optional[datetime] = Query(None, description="æ—¥å¿—ç»“æŸæ—¶é—´ï¼ˆå¦‚ 2025-09-19T12:00:00ï¼‰"),
    limit: int = Query(100, description="æœ€å¤šè¿”å›æ¡æ•°(<=1000)"),
    api_key: str = Depends(verify_api_key)
) ->Dict[str, Any]:
    """
    è·å–çˆ¬è™«æ—¥å¿—ï¼ˆæ”¯æŒæ—¶é—´èŒƒå›´ç­›é€‰ï¼‰
    - æ—¥å¿—æ ¼å¼ä¸ notification.py çš„ log_post_info ä¸€è‡´
    - æŒ‰æ—¶é—´å€’åºè¿”å›ï¼ˆæœ€æ–°æ—¥å¿—åœ¨å‰ï¼‰
    """
    limit = min(limit, 100)
    log_file = API_CONFIG["log_file_path"]

    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(log_file):
        return {
            "code": 200,
            "message": "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨",
            "data": {"total": 0, "logs": []}
        }

    # è¯»å–æ—¥å¿—æ–‡ä»¶
    logs = []
    with open (log_file, "r", encoding="utf-8") as f:
        current_log = ""
        for line in f:
            # æ—¥å¿—è¾¹ç•Œè¯†åˆ«ï¼šä»¥ "[YYYY-MM-DD HH:MM:SS]" å¼€å¤´çš„è¡Œä¸ºæ–°æ—¥å¿—
            if line.startswith("[") and len(line) >=20:
                if current_log:
                    logs.append(current_log.strip())
                current_log = line # æŠŠ current_logé‡ç½®ä¸ºå½“å‰è¡Œï¼ˆæ–°æ—¥å¿—çš„å¼€å¤´ï¼‰
            else:
                current_log += line
        if current_log:
            logs.append(current_log.strip())
    
    # æ—¶é—´èŒƒå›´ç­›é€‰
    filtered_logs = []
    for log in logs:
        # æå–æ—¥å¿—æ—¶é—´æˆ³ æ ¼å¼ï¼š[2024-10-01 12:00:00]
        if not log.startswith("["):
            continue
        log_time_str = log[1:20] 
        try:
            log_time = datetime.strptime(log_time_str,"%Y-%m-%d %H:%M:%S")
        except:
            continue  # è·³è¿‡æ ¼å¼å¼‚å¸¸çš„æ—¥å¿—

        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if start_time and log_time < start_time: # æ—©äºç”¨æˆ·éœ€æ±‚å¼€å§‹å‰çš„å»é™¤
            continue
        if end_time and log_time > end_time : # æ™šäºç”¨æˆ·ç»“æŸæ—¶é—´çš„å»é™¤
            continue
        filtered_logs.append(
            {
                "log_time": log_time_str,
                "content": log
            }
        )

    # æŒ‰æ—¶é—´å€’å™,å–å‰limitæ¡
    filtered_logs.sort(key=lambda x: x["log_time"], reverse=True)
    return {
        "code": 200,
        "message": "success",
        "data": {
            "total": len(filtered_logs),
            "limit": limit,
            "logs": filtered_logs[:limit] # è¿™é‡Œæ·»åŠ [::-1]å¯ä»¥å°†limitæ–°åˆ°æœ€æ–°æŒ‰æ—¶é—´é¡ºåº
        }
    }

@app.get("/api/anti-crawl/ip-pool", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-IPæ± "])
def get_ip_pool_status_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """è·å–IPæ± çŠ¶æ€ï¼ˆç»Ÿè®¡ä¿¡æ¯+IPè¯¦æƒ…ï¼‰"""
    return {
        "code": 200,
        "message": "success! æˆåŠŸè·å–åçˆ¬ç­–ç•¥",
        "data": ip_pool.get_pool_status()
    }

@app.post("/api/anti-crawl/ip-pool/add", tags=["3.åçˆ¬ç­–ç•¥ç®¡ç†-IPæ± "])  # æ·»åŠ æ–°ipåˆ°IPæ± 
def add_ip_api(
    req: IPAddRequest,
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """æ·»åŠ IPåˆ°IPæ± ï¼ˆè‡ªåŠ¨æ£€æµ‹æœ‰æ•ˆæ€§ï¼Œé‡å¤/æ— æ•ˆIPä¼šè¿”å›å¤±è´¥ï¼‰"""
    # æ ¡éªŒIPæ ¼å¼ï¼ˆå¿…é¡»åŒ…å« ":"ï¼Œä¸”ç«¯å£ä¸ºæ•°å­—ï¼‰
    if ":" not in req.ip: # è¿˜æœ‰protocol
        raise HTTPException(status_code=400, detail="IPæ ¼å¼é”™è¯¯ï¼Œéœ€ä¸º 'ip:port'ï¼ˆå¦‚ 1.2.3.4:7891ï¼‰")
    ip_parts = req.ip.split(":")
    if not ip_parts[1].isdigit():
        raise HTTPException(status_code=400, detail="IPç«¯å£å¿…é¡»ä¸ºæ•°å­—ï¼ˆå¦‚ 1.2.3.4:7891ï¼‰")
    # è°ƒç”¨åçˆ¬æ¨¡å—æ·»åŠ IP
    success = ip_pool.add_ip(req.ip, req.protocol) #
    if success:
        return {
            "code":200,
            "message": f"IP {req.ip} æ·»åŠ æˆåŠŸï¼ˆå·²éªŒè¯æœ‰æ•ˆï¼‰", # åçˆ¬æ¨¡å—çš„add_ipå‡½æ•°åŒ…å«éªŒè¯ipæœ‰æ•ˆéƒ¨åˆ†
            "data": {}
        }
    raise HTTPException(status_code=400, detail=f"IP {req.ip} æ— æ•ˆæˆ–å·²åœ¨æ± ä¸­")

@app.post("/api/anti-crawl/ip-pool/remove", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-IPæ± "])
def remove_ip_api(
    ip: str = Query(..., description="å¾…åˆ é™¤IP(æ ¼å¼ï¼š ip:port)"),
    api_key: str = Depends(verify_api_key)
) ->  Dict[str, Any]:
    """ä»IPæ± åˆ é™¤æŒ‡å®šIP"""
    success = ip_pool.remove_ip(ip)
    if success:
        return {
            "code": 200,
            "message": f"IP {ip} å·²ä»æ± ä¸­åˆ é™¤",
            "data": {}
        }
    raise HTTPException(status_code=404, detail=f"IP {ip} ä¸åœ¨æ± ä¸­ï¼Œåˆ é™¤å¤±è´¥")

@app.get("/api/anti-crawl/strategy", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-æ™ºèƒ½å‚æ•°"])
def get_strategy_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """è·å–å½“å‰ç”Ÿæ•ˆçš„æ™ºèƒ½åçˆ¬ç­–ç•¥ï¼ˆå¹¶å‘/é—´éš”/IPåˆ‡æ¢ç­‰å‚æ•°ï¼‰"""
    return {
        "code": 200,
        "message": "success!æˆåŠŸè·å–æ¨¡å‹æ™ºèƒ½åçˆ¬ç­–ç•¥",
        "data": smart_strategy.get_current_strategy()
    }

@app.post("/api/anti-crawl/strategy/update", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-æ™ºèƒ½å‚æ•°"])
def update_strategy_api(
    req: StrategyUpdateRequest, # FastAPI ä¼šè‡ªåŠ¨å°† ç”¨æˆ·é€šè¿‡APIå‘é€çš„JSON è¯·æ±‚æ•°æ®è§£æåˆ° StrategyUpdateRequest æ¨¡å‹
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    """
    æ›´æ–°æ™ºèƒ½åçˆ¬ç­–ç•¥ï¼ˆä»…å…è®¸ä¿®æ”¹é¢„è®¾å­—æ®µï¼‰
    - å‚æ•°ä¼šè‡ªåŠ¨æ ¡éªŒåˆæ³•æ€§ï¼ˆå¦‚å¹¶å‘é™åˆ¶ 1~100ï¼Œé—´éš” 1~60ç§’ï¼‰
    """
    new_params = req.model_dump(exclude_none=True) # æ’é™¤ç©ºå€¼ 
    if not new_params:
        raise HTTPException(status_code=400, detail= "éœ€è‡³å°‘ä¼ å…¥ä¸€ä¸ªå¾…æ›´æ–°çš„ç­–ç•¥å‚æ•°")
    
    # è°ƒç”¨åçˆ¬æ¨¡å—æ›´æ–°ç­–ç•¥
    updated_strategy = smart_strategy.update_strategy(new_params)
    return {
        "code": 200,
        "message": "ç­–ç•¥æ›´æ–°æˆåŠŸ",
        "data": updated_strategy
    }

# ---------------------- 6. APIæœåŠ¡å¯åŠ¨å…¥å£ ----------------------
if __name__ == "__main__":
    print("="*60)
    print("         Redditçˆ¬è™«APIæœåŠ¡å¯åŠ¨ä¸­         ")
    print("="*60)
    print(f" APIåœ°å€ï¼šhttp://{API_CONFIG['host']}:{API_CONFIG['port']}")
    print(f" æ¥å£æ–‡æ¡£ï¼šhttp://{API_CONFIG['host']}:{API_CONFIG['port']}/docs")
    print(f" APIå¯†é’¥ï¼š{API_CONFIG['api_key']}ï¼ˆè¯·æ±‚æ—¶éœ€æºå¸¦ï¼‰")
    print(f" åçˆ¬æ¨¡å—ï¼šå·²åŠ è½½ IPæ± ï¼ˆåˆå§‹æœ‰æ•ˆIPï¼š{ip_pool.get_pool_status()['statistics']['valid_ip_count']}ï¼‰")
    print("="*60)
    
    # å¯åŠ¨FastAPIæœåŠ¡ ï¼ˆä½¿ç”¨Uvicornï¼Œæ”¯æŒé«˜å¹¶å‘ï¼‰
    uvicorn.run(
        app="__main__:app",
        host=API_CONFIG["host"],
        port=API_CONFIG["port"],
        workers=1, # å·¥ä½œè¿›ç¨‹æ•°ï¼ˆå»ºè®®ä¸ºCPUæ ¸å¿ƒæ•°çš„2å€ï¼Œç”Ÿäº§ç¯å¢ƒå¯è°ƒæ•´ï¼‰
        reload=False # ç”Ÿäº§ç¯å¢ƒå…³é—­çƒ­é‡è½½ï¼ˆé¿å…æ€§èƒ½æŸè€—ï¼‰
    )