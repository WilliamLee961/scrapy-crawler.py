# è¯¥pythonæ–‡ä»¶çš„ä»»åŠ¡æ˜¯å®Œæˆçˆ¬è™«ç­–ç•¥çš„apiæ¥å£
import os
import time
import json
import pandas as pd
import csv
from pprint import pprint
from fastapi.responses import JSONResponse
import traceback
import threading
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from fastapi import FastAPI, HTTPException, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import sqlite3
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)
from skool_crawl.skool_crawler_doubao import summarize_with_doubao
from volcenginesdkarkruntime import Ark 
from reddit_crawler import RedditCrawler
from notification import (
    load_processed_posts, 
    save_processed_posts,
    log_post_info
)
from anti_crawl_core import ip_pool, smart_strategy
from dotenv import load_dotenv
import mysql.connector
from mysql.connector import errorcode
# åŠ è½½ä¸Šçº§ç›®å½•çš„ .env
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env"))
MYSQL_URL = os.getenv("MYSQL_URL")
NEO4J_URI = os.getenv("NEO4J_URI")


# ---------------------- 1. å…¨å±€é…ç½®ä¸åˆå§‹åŒ– ----------------------
# APIæœåŠ¡é…ç½®
API_CONFIG = {
    "host": "127.0.0.1",
    "port": 8000,
    "api_key": "RedditCrawler_2024",
    "log_file_path": "reddit_posts.log",
    "max_concurrent": 100,
    "doubao_api_key": "165e659b-a12e-462d-8398-68da89fbcebb",  # æ›¿æ¢ä¸ºå®é™…APIå¯†é’¥
    "doubao_base_url": "https://ark.cn-beijing.volces.com/api/v3",  # å®˜æ–¹base_url
    "doubao_model": "doubao-1-5-pro-32k-250115"  # å®˜æ–¹æŒ‡å®šæ¨¡å‹
}

try:
    doubao_client = Ark(
        base_url=API_CONFIG["doubao_base_url"],
        api_key=API_CONFIG["doubao_api_key"]
    )
except Exception as e:
    print(f"è±†åŒ…å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    doubao_client = None

# åˆå§‹åŒ–FastApiåº”ç”¨
app = FastAPI(
    title="Redditçˆ¬è™«å¯¹å¤–APIæœåŠ¡",
    description="åŠŸèƒ½ï¼šå®ç°çˆ¬è™«çŠ¶æ€æŸ¥è¯¢ã€æ—¥å¿—è¯»å–ã€åçˆ¬ç­–ç•¥é…ç½®ï¼ˆIPæ± /æ™ºèƒ½ç­–ç•¥ï¼‰",
    version="1.0.0",
    default_response_class=JSONResponse,
    responses={
        200: {
            "description": "è¯·æ±‚æˆåŠŸ",
            "content": {
                "application/json": {
                    "charset": "utf-8",
                    "example": {
                        "code": 200,
                        "message": "æ“ä½œæˆåŠŸ",
                        "data": {}
                    }
                }
            }
        },
        400: {
            "description": "è¯·æ±‚å‚æ•°é”™è¯¯",
            "content": {
                "application/json": {
                    "charset": "utf-8",
                    "example": {
                        "detail": "å‚æ•°é”™è¯¯ï¼Œéœ€è‡³å°‘ä¼ å…¥ä¸€ä¸ªå­—æ®µ"
                    }
                }
            }
        }
    }
)

# è§£å†³è·¨åŸŸé—®é¢˜
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
NEW_DB_PATH = "reddit_test_posts.db"  # æ–°DBæ–‡ä»¶
NEW_CSV_PATH = "reddit_test_posts.csv"  # æ–°CSVæ–‡ä»¶
# ---------------------- 2. è±†åŒ…APIè°ƒç”¨å·¥å…·å‡½æ•°ï¼ˆæŒ‰å®˜æ–¹æ–‡æ¡£å®ç°ï¼‰ ----------------------
def normalize_posts_to_content(posts: List[Dict]) -> List[Dict]:
    """
    ç¡®ä¿æ¯æ¡ post éƒ½æœ‰ç»Ÿä¸€çš„ 'content' å­—æ®µï¼ˆä¼˜å…ˆçº§æŒ‰ä¸‹åˆ— keysï¼‰ã€‚
    åŒæ—¶ç¡®ä¿æœ‰ excerpt ä¸ fetched_at å­—æ®µï¼Œé¿å…ç©ºå†…å®¹å¯¼è‡´ downstream é—®é¢˜ã€‚
    å¹¶æ‰“å° debug ä¿¡æ¯ç”¨äºå®šä½é—®é¢˜å¸–ã€‚
    """
    if not posts:
        return posts

    possible_keys = ["content", "self_text", "body", "text", "self_text_html", "raw_text"]
    normalized = []
    for idx, p in enumerate(posts):
        p = dict(p)  # shallow copy é˜²æ­¢å‰¯ä½œç”¨
        found = False
        for k in possible_keys:
            val = p.get(k)
            if val and isinstance(val, str) and val.strip():
                p["content"] = val.strip()
                found = True
                break
        if not found:
            fallback = ""
            if p.get("excerpt"):
                fallback = str(p.get("excerpt"))
            elif p.get("title"):
                fallback = "æ ‡é¢˜: " + str(p.get("title"))
            elif p.get("url"):
                fallback = "é“¾æ¥: " + str(p.get("url"))
            else:
                fallback = ""
            p["content"] = fallback

            print(f"[normalize] post #{idx} æ²¡æœ‰æ ‡å‡†æ­£æ–‡å­—æ®µï¼Œå·²ç”¨ fallback å¡«å……ï¼ˆlen={len(fallback)})ï¼ŒåŸ keys: {list(p.keys())}")

        content_for_excerpt = p.get("content", "") or ""
        p["excerpt"] = p.get("excerpt") or (content_for_excerpt[:150] + "..." if len(content_for_excerpt) > 150 else content_for_excerpt)

        # fetched_at ä¿åº•
        p["fetched_at"] = p.get("fetched_at") or datetime.now().isoformat()
        normalized.append(p)
    return normalized

def get_post_summary(text: str) -> str:
    """è°ƒç”¨è±†åŒ…APIè·å–å¸–å­ç»¼è¿°ï¼ˆä¸¥æ ¼éµå¾ªå®˜æ–¹SDKè°ƒç”¨æ–¹å¼ï¼‰"""
    if not doubao_client:
        return "è±†åŒ…å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥"
    
    if not text.strip():
        return "å¸–å­å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆç»¼è¿°"
    
    try:
        # è°ƒç”¨è±†åŒ…å®˜æ–¹SDKçš„chat.completions.createæ–¹æ³•
        completion = doubao_client.chat.completions.create(
            model=API_CONFIG["doubao_model"],
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸­æ–‡ç§‘æŠ€æ–‡ç« ç»¼è¿°ä¸“å®¶"},
                {"role": "user", "content": text}
            ]
        )
        # ä»è¿”å›ç»“æœä¸­æå–å†…å®¹ï¼ˆæŒ‰å®˜æ–¹å“åº”æ ¼å¼ï¼‰
        return completion.choices[0].message.content
    except Exception as e:
        return f"è±†åŒ…APIè°ƒç”¨å¤±è´¥: {str(e)}"

# ---------------------- 3. çˆ¬è™«çŠ¶æ€ç®¡ç† ----------------------
class CrawlerState:
    """çˆ¬è™«è¿è¡ŒçŠ¶æ€ç®¡ç†ï¼ˆç›‘æ§å¹¶å‘ã€å»¶è¿Ÿã€æ¨é€å“åº”æ—¶é—´ï¼Œä¾èµ–åçˆ¬æ¨¡å—ï¼‰"""
    def __init__(self):
        self.state = {
            "is_running": False,
            "current_concurrent": 0,
            "recent_crawl_delays": [],
            "last_push_response_time": None,
            "total_crawled_posts": 0,
            "total_pushed_posts": 0
        }
        self.lock = threading.Lock()
        self.crawler_thread: Optional[threading.Thread] = None
        self.crawled_results = []
        self.results_lock = threading.Lock()

    def update_crawl_delay(self, delay: float):
        """æ›´æ–°é‡‡é›†å»¶è¿Ÿï¼ˆä¿ç•™æœ€è¿‘10æ¬¡æ•°æ®ï¼‰"""
        with self.lock:
            self.state["recent_crawl_delays"].append(round(delay, 2))
            if len(self.state["recent_crawl_delays"]) > 10:
                self.state["recent_crawl_delays"].pop(0)
    
    def update_push_response_time(self, response_time: float):
        """æ›´æ–°æ¨é€å“åº”æ—¶é—´"""
        with self.lock:
            self.state["last_push_response_time"] = round(response_time, 2)
            self.state["total_pushed_posts"] += 1
    
    def increment_concurrent(self) -> bool:
        """å¢åŠ å¹¶å‘ä¼šè¯æ•°"""
        with self.lock:
            strategy = smart_strategy.get_current_strategy()
            if self.state["current_concurrent"] < strategy["concurrent_limit"]:
                self.state["current_concurrent"] += 1
                return True
            print(f" å¹¶å‘æ•°å·²è¾¾ä¸Šé™ï¼ˆ{strategy['concurrent_limit']}ï¼‰ï¼Œæ— æ³•æ–°å¢ä¼šè¯")
            return False
    
    def decrement_concurrent(self):
        """å‡å°‘å¹¶å‘ä¼šè¯æ•°"""
        with self.lock:
            if self.state["current_concurrent"] > 0:
                self.state["current_concurrent"] -= 1
                if self.state["current_concurrent"] == 0:
                    print(" å½“å‰å¹¶å‘ä¼šè¯æ•°å·²å‡è‡³0")

    def test_single_crawl(self) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡çˆ¬å–æµ‹è¯•ï¼ˆæ±‡æ€»æ‰€æœ‰å¸–å­æ­£æ–‡ç”Ÿæˆæ•´ä½“ç»¼è¿°ï¼‰"""
        try:
            print("\n" + "=" * 60)
            print(" æ‰‹åŠ¨è§¦å‘å•æ¬¡çˆ¬å–æµ‹è¯•ï¼ˆæ•´ä½“ç»¼è¿°ç‰ˆï¼Œæ— ç¤¾ç¾¤æ³›åŒ–ï¼‰")
            print("=" * 60)

            # è·å–ä»£ç†ä¸ç­–ç•¥
            ip_status = ip_pool.get_pool_status()["statistics"]
            strategy = smart_strategy.get_current_strategy()
            PROXY_HOST = PROXY_PORT = None

            if ip_status["valid_ip_count"] > 0:
                selected_ip = ip_pool.get_random_valid_ip()
                if selected_ip:
                    ip_parts = selected_ip["ip"].split(":")
                    PROXY_HOST, PROXY_PORT = ip_parts[0], int(ip_parts[1])
                    print(f"ä½¿ç”¨ä»£ç†IP: {selected_ip['ip']} ({selected_ip['protocol']})")
            else:
                print("æœªæ‰¾åˆ°å¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼")

            # å®ä¾‹åŒ– Reddit çˆ¬è™«
            crawler = RedditCrawler(proxy_host=PROXY_HOST, proxy_port=PROXY_PORT)
            time_threshold = datetime.now() - timedelta(hours=24)
            subreddit = strategy.get("target_subreddit", "python")
            print(f"å¼€å§‹çˆ¬å–å­ç‰ˆå—ï¼š{subreddit}")

            # æ‰§è¡Œçˆ¬å–
            new_posts = crawler.get_new_posts(
                subreddit_name=subreddit,
                limit=10,
                max_comments=0,
                time_threshold=time_threshold.timestamp()
            )

            if not new_posts:
                print("âš ï¸ æœªè·å–åˆ°ä»»ä½•å¸–å­")
                return {
                    "success": False,
                    "crawled_count": 0,
                    "posts": [],
                    "message": f"å­ç‰ˆå— {subreddit} æœªè·å–åˆ°æ–°å¸–å­"
                }

            # æ•´ç†å­—æ®µä¸æ­£æ–‡
            for p in new_posts:
                p["content"] = p.get("content", "")
                p["excerpt"] = (p["content"][:150] + "...") if len(p["content"]) > 150 else p["content"]
                p["fetched_at"] = datetime.now().isoformat()

            # ä¿å­˜ç»“æœï¼ˆCSV + SQLiteï¼‰
            save_posts_to_sqlite(new_posts)
            save_posts_to_csv(new_posts)

            # è°ƒç”¨ summarize_with_doubao ç”Ÿæˆæ•´ä½“ç»¼è¿°
            print(" è°ƒç”¨ summarize_with_doubao ç”Ÿæˆæ•´ä½“ä¸­æ–‡ç»¼è¿°...")
            summary_result = summarize_with_doubao(
                posts=new_posts,
                doubao_key=API_CONFIG["doubao_api_key"],
                model=API_CONFIG["doubao_model"]
            )
            summary_text = summary_result.get("summary", "").strip()
            if not summary_text:
                summary_text = "ï¼ˆç»¼è¿°ç”Ÿæˆå¤±è´¥æˆ–å†…å®¹ä¸ºç©ºï¼‰"

            summary_path = f"reddit_summary_{subreddit}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(summary_path, "w", encoding="utf-8") as f:
                f.write(summary_text)
            print(f"å¸–å­ç»¼è¿°å·²ä¿å­˜åˆ° {summary_path}")

            self.add_crawled_result(new_posts)
            result = {
                "success": True,
                "crawled_count": len(new_posts),
                "posts": new_posts,
                "summary_file": summary_path,
                "message": f"æˆåŠŸçˆ¬å– {len(new_posts)} æ¡å¸–å­ï¼Œå¹¶ç”Ÿæˆæ•´ä½“ä¸­æ–‡ç»¼è¿°"
            }

            pprint("\nğŸ¯ ç»¼åˆç»¼è¿°è¾“å‡ºé¢„è§ˆï¼š")
            print(summary_text[:500] + "..." if len(summary_text) > 500 else summary_text)
            return result

        except Exception as e:
            traceback.print_exc()
            return {
                "success": False,
                "crawled_count": 0,
                "posts": [],
                "message": f"æµ‹è¯•æ‰‹åŠ¨çˆ¬å–å¤±è´¥ï¼š{str(e)}"
            }

    def start_crawler(self) -> bool:
        """å¯åŠ¨çˆ¬è™«ï¼ˆåå°çº¿ç¨‹è¿è¡Œï¼‰"""
        with self.lock:
            if self.state["is_running"]:
                print(" çˆ¬è™«å·²ç»åœ¨è¿è¡Œï¼Œæ— éœ€é‡å¤å¯åŠ¨ï¼")
                return False
            self.crawler_thread = threading.Thread(
                target=self._crawler_main_loop,
                daemon=True
            )
            self.crawler_thread.start()
            self.state["is_running"] = True
            print(" çˆ¬è™«å·²å¯åŠ¨ï¼ˆåå°çº¿ç¨‹ï¼‰")
            return True

    def add_crawled_result(self, posts):
        with self.results_lock:
            self.crawled_results.extend(posts)
            if len(self.crawled_results) > 1000:
                self.crawled_results = self.crawled_results[-1000:]

    def get_crawled_posts(self, limit=100):
        with self.results_lock:
            return self.crawled_results[-limit:]

    def stop_crawler(self) -> bool:
        """åœæ­¢çˆ¬è™«"""
        with self.lock:
            if not self.state["is_running"]:
                print(" çˆ¬è™«å·²åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ")
                return False
            self.state["is_running"] = False
            if self.crawler_thread and self.crawler_thread.is_alive():
                self.crawler_thread.join(timeout=5)
                if self.crawler_thread.is_alive():
                    print("çˆ¬è™«çº¿ç¨‹æœªåŠæ—¶é€€å‡ºï¼Œå¯èƒ½å­˜åœ¨æœªå®Œæˆä»»åŠ¡")  
            save_processed_posts(load_processed_posts())
            print(" çˆ¬è™«å·²åœæ­¢ï¼Œå·²ä¿å­˜å¤„ç†è®°å½•")
            return True
        
    def _crawler_main_loop(self):
        """åå°çˆ¬è™«ä¸»å¾ªç¯ï¼ˆæ‰¹é‡çˆ¬å– + æ•´ä½“ç»¼è¿°ç”Ÿæˆï¼‰"""
        print(f"[åå°çˆ¬è™«] çº¿ç¨‹å·²å¯åŠ¨ï¼Œè¿›å…¥å¾ªç¯ï¼ˆis_running: {self.state['is_running']}ï¼‰")

        while self.state["is_running"]:
            print(f"\n[åå°çˆ¬è™«] è¿›å…¥å¾ªç¯è¿­ä»£ï¼ˆå½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%H:%M:%S')}ï¼‰")
            try:
                # ---------- Step 1: åˆå§‹åŒ–å‚æ•° ----------
                PROXY_HOST = PROXY_PORT = None
                ip_status = ip_pool.get_pool_status()["statistics"]

                if ip_status["valid_ip_count"] > 0:
                    if smart_strategy.need_auto_switch_ip():
                        selected_ip = ip_pool.get_random_valid_ip()
                        print(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢IPï¼š{selected_ip['ip']}")
                    else:
                        selected_ip = ip_pool.get_current_ip() or ip_pool.get_random_valid_ip()
                        print(f"ä½¿ç”¨å½“å‰IPï¼š{selected_ip['ip']}")
                    if selected_ip:
                        ip_parts = selected_ip["ip"].split(":")
                        PROXY_HOST, PROXY_PORT = ip_parts[0], int(ip_parts[1])
                        print(f"ä»£ç†è®¾ç½®ï¼š{PROXY_HOST}:{PROXY_PORT}")
                else:
                    print("[åå°çˆ¬è™«] æ— æœ‰æ•ˆä»£ç†ï¼Œä½¿ç”¨ç›´è¿")

                # ---------- Step 2: æ£€æŸ¥å¹¶å‘é™åˆ¶ ----------
                if not self.increment_concurrent():
                    print("[åå°çˆ¬è™«] å¹¶å‘å·²æ»¡ï¼Œç­‰å¾… 1 ç§’...")
                    time.sleep(1)
                    continue
                print(f"[åå°çˆ¬è™«] å¹¶å‘æ•° +1ï¼Œå½“å‰å¹¶å‘æ•°ï¼š{self.state['current_concurrent']}")

                # ---------- Step 3: æ‰§è¡Œçˆ¬å– ----------
                strategy = smart_strategy.get_current_strategy()
                subreddit = strategy.get("target_subreddit", "python")
                print(f"[åå°çˆ¬è™«] å¼€å§‹çˆ¬å–å­ç‰ˆå—ï¼š{subreddit}")

                start_time = time.time()
                time_threshold = datetime.now() - timedelta(hours=24)

                crawler = RedditCrawler(proxy_host=PROXY_HOST, proxy_port=PROXY_PORT)
                new_posts = crawler.get_new_posts(
                    subreddit_name=subreddit,
                    limit=10,
                    max_comments=0,
                    time_threshold=time_threshold.timestamp()
                )

                if not new_posts:
                    print(f"[åå°çˆ¬è™«] æœªè·å–åˆ°æ–°å¸–å­ï¼Œç­‰å¾… {strategy['crawl_interval']} ç§’åé‡è¯•")
                    self.decrement_concurrent()
                    time.sleep(strategy["crawl_interval"])
                    continue

                for p in new_posts:
                    p["content"] = p.get("content", "")
                    p["excerpt"] = (p["content"][:150] + "...") if len(p["content"]) > 150 else p["content"]
                    p["fetched_at"] = datetime.now().isoformat()

                # ---------- Step 4: ä¿å­˜åŸºç¡€å¸–å­ ----------
                save_posts_to_sqlite(new_posts)
                save_posts_to_csv(new_posts)
                init_mysql_table()
                save_posts_to_mysql(new_posts)

                # ---------- Step 5: ç”Ÿæˆæ•´ä½“ç»¼è¿° ----------
                print(f"[åå°çˆ¬è™«] è°ƒç”¨ summarize_with_doubao ç”Ÿæˆæ•´ä½“ä¸­æ–‡ç»¼è¿°ï¼ˆ{len(new_posts)} æ¡ï¼‰...")
                summary_result = summarize_with_doubao(
                    posts=new_posts,
                    doubao_key=API_CONFIG["doubao_api_key"],
                    model=API_CONFIG["doubao_model"]
                )
                # åŸè¦æ±‚ï¼šä¿å­˜ä¸ºtxtæ ¼å¼æ–‡ä»¶
                summary_text = summary_result.get("summary", "").strip() or "ï¼ˆç»¼è¿°ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©ºï¼‰"
                summary_path = f"reddit_summary_{subreddit}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                with open(summary_path, "w", encoding="utf-8") as f:
                    f.write(summary_text)
                print(f" å·²ä¿å­˜ç»¼è¿°æ–‡ä»¶ï¼š{summary_path}")

                # æ˜“è€å¸ˆæ–°è¦æ±‚ï¼šä¿å­˜ä¸ºMarkdown + Word
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                subreddit = strategy.get("target_subreddit", "reddit")
                base_name = f"reddit_summary_{subreddit}_{timestamp}"
                # ç”Ÿæˆ Markdown æ–‡ä»¶
                markdown_text = f"""# Reddit å­ç‰ˆå—ç»¼åˆç»¼è¿°ï¼š{subreddit}
                ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

                ---

                ## æ‘˜è¦
                {summary_text}

                ---

                ## æ•°æ®ç»Ÿè®¡
                - å¸–å­æ€»æ•°ï¼š{len(new_posts)}
                - æ•°æ®æ¥æºï¼šsubreddit r/{subreddit}
                - æ¨¡å‹ï¼šDoubaoï¼ˆ{API_CONFIG["doubao_model"]}ï¼‰

                """
                md_path = f"{base_name}.md"
                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(markdown_text)
                print(f" å·²ç”Ÿæˆ Markdown æ–‡ä»¶ï¼š{md_path}")
                
                # è½¬æ¢ä¸º Word æ–‡ä»¶
                try:
                    import pypandoc
                    docx_path = f"{base_name}.docx"
                    pypandoc.convert_text(markdown_text, "docx", format="md", outputfile=docx_path, extra_args=["--standalone"])
                    print(f" å·²ç”Ÿæˆ Word æ–‡ä»¶ï¼š{docx_path}")
                except Exception as e:
                    print(f" ç”Ÿæˆ Word æ–‡ä»¶å¤±è´¥ï¼š{e}")

                # ---------- Step 6: æ›´æ–°è¿è¡ŒçŠ¶æ€ ----------
                self.add_crawled_result(new_posts)
                crawl_delay = time.time() - start_time
                self.update_crawl_delay(crawl_delay)
                self.state["total_crawled_posts"] += len(new_posts)

                print(f"[åå°çˆ¬è™«] æœ¬è½®å®Œæˆï¼šé‡‡é›† {len(new_posts)} æ¡ï¼Œç”¨æ—¶ {crawl_delay:.2f}s")
                print(f"[åå°çˆ¬è™«] ç»¼è¿°æ‘˜è¦é¢„è§ˆï¼š{summary_text[:300]}...")

                # ---------- Step 7: ç­‰å¾…ä¸‹ä¸€è½® ----------
                self.decrement_concurrent()
                interval = strategy.get("crawl_interval", 60)
                print(f"[åå°çˆ¬è™«] ç­‰å¾… {interval} ç§’è¿›å…¥ä¸‹ä¸€è½®...\n")
                time.sleep(interval)

            except Exception as e:
                traceback.print_exc()
                self.decrement_concurrent()
                print(f"[åå°çˆ¬è™«] å¼‚å¸¸ï¼š{str(e)}ï¼Œä¼‘çœ  5 ç§’é‡è¯•...\n")
                time.sleep(5)

    def get_current_state(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«å®æ—¶çŠ¶æ€"""
        with self.lock:
            delays = self.state["recent_crawl_delays"]
            avg_delay = round(sum(delays)/ len(delays), 2) if delays else 0.0

            return {
                "basic_status": {
                    "is_running": self.state["is_running"],
                    "current_concurrent": self.state["current_concurrent"],
                    "total_crawled_posts": self.state["total_crawled_posts"],
                    "total_pushed_posts": self.state["total_pushed_posts"]
                },

                "performance_metrics": {
                    "recent_crawl_delays": self.state["recent_crawl_delays"],
                    "avg_crawl_delay": avg_delay,
                    "last_push_response_time": self.state["last_push_response_time"],
                    "is_crawl_delay_qualified": avg_delay <= 3,
                    "is_push_qualified": (
                        self.state["last_push_response_time"] is None
                        or self.state["last_push_response_time"] <= 120
                    )
                }
            }

# åˆå§‹åŒ–çˆ¬è™«çŠ¶æ€ç®¡ç†
crawler_state = CrawlerState()

def save_posts_to_csv(posts: List[Dict], csv_path: str = NEW_CSV_PATH):
    """ä¿å­˜å¸–å­åˆ°CSVï¼Œå­—æ®µä¸SQLiteè¡¨å®Œå…¨å¯¹é½"""
    # å®šä¹‰ä¸SQLiteè¡¨ä¸€è‡´çš„å­—æ®µåˆ—è¡¨ï¼ˆé¡ºåºä¹Ÿä¿æŒä¸€è‡´ï¼‰
    posts = normalize_posts_to_content(posts)
    fields = [
        "id", "url", "title", "author", "time", 
        "likes", "comments", "excerpt", "content", "fetched_at"
    ]
    
    file_exists = os.path.exists(csv_path)
    
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        
        if not file_exists:
            writer.writeheader()
        
        for post in posts:
            fetched_at = datetime.now().isoformat()
            
            # æ„é€ ä¸SQLiteå­—æ®µå¯¹åº”çš„è¡Œæ•°æ®
            row = {
                "id": post.get("id", ""),
                "url": post.get("url", ""),
                "title": post.get("title", ""),
                "author": post.get("author", ""),
                "time": post.get("time", "") or post.get("created_utc", ""),
                "likes": post.get("likes", post.get("score", 0)) or 0,
                "comments": post.get("comments", post.get("num_comments", 0)) or 0,
                "excerpt": post.get("excerpt", ""),
                "content": post.get("content", ""),
                "fetched_at": fetched_at
            }
            
            writer.writerow(row)
    
    if posts:
        print(f"[save_posts_to_csv] å·²ä¿å­˜ {len(posts)} æ¡åˆ° {csv_path}ï¼›ç¬¬ä¸€æ¡ content ç‰‡æ®µï¼š{posts[0].get('content','')[:200]}")
    else:
        print(f"[save_posts_to_csv] æ— å¸–å­å¯ä¿å­˜åˆ° {csv_path}")


# å¤ç”¨ä½ çš„save_posts_to_sqliteï¼Œä½†é»˜è®¤ä¿å­˜åˆ°æ–°DB
def save_posts_to_sqlite(posts: List[Dict], db_path: str = NEW_DB_PATH):
    posts = normalize_posts_to_content(posts)
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    c.execute("""
    CREATE TABLE IF NOT EXISTS posts (
        db_id INTEGER PRIMARY KEY AUTOINCREMENT,
        reddit_id TEXT UNIQUE,
        url TEXT UNIQUE,
        title TEXT,
        author TEXT,
        time TEXT,
        likes INTEGER,
        comments INTEGER,
        excerpt TEXT,
        content TEXT,
        fetched_at TEXT
    )
    """)
    c.execute("PRAGMA table_info(posts)")
    cols = [row[1] for row in c.fetchall()]
    if "content" not in cols:
        print("[DB] æ£€æµ‹åˆ°æ—§ç‰ˆæ•°æ®åº“ç»“æ„ï¼Œè‡ªåŠ¨æ·»åŠ  content åˆ—...")
        c.execute("ALTER TABLE posts ADD COLUMN content TEXT")
        conn.commit()
    for post in posts:
        c.execute("""
            INSERT OR REPLACE INTO posts 
            (reddit_id, url, title, author, time, likes, comments, excerpt, content, fetched_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
            post.get("id", ""),
            post.get("url", ""),
            post.get("title", ""),
            post.get("author", ""),
            post.get("time", "") or post.get("created_utc", ""),
            post.get("likes", post.get("score", 0)) or 0,
            post.get("comments", post.get("num_comments", 0)) or 0,
            post.get("excerpt", ""),
            post.get("content", ""),
            post.get("fetched_at", datetime.now().isoformat())
        ))
    conn.commit()
    conn.close()
    print(f"[save_posts_to_sqlite] å·²ä¿å­˜ {len(posts)} æ¡åˆ° {db_path}ï¼›ç¬¬ä¸€æ¡ content ç‰‡æ®µï¼š{posts[0].get('content','')[:200]}")

import pymysql
from sqlalchemy import create_engine, text

def init_mysql_table():
    """åˆå§‹åŒ– MySQL reddit_posts è¡¨"""
    try:
        engine = create_engine(MYSQL_URL)
        with engine.connect() as conn:
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS reddit_posts (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    reddit_id VARCHAR(50) UNIQUE,
                    url TEXT UNIQUE,
                    title TEXT,
                    author VARCHAR(255),
                    time VARCHAR(255),
                    excerpt TEXT,
                    content LONGTEXT,
                    fetched_at VARCHAR(255)
                ) CHARACTER SET utf8mb4;
            """))
            print("[MySQL] reddit_posts è¡¨å·²åˆå§‹åŒ–")
    except Exception as e:
        print(f"[MySQL] åˆå§‹åŒ–å¤±è´¥: {e}")

def save_posts_to_mysql(posts: List[Dict]):
    """ä¿å­˜å¸–å­åˆ° MySQL æ•°æ®åº“ crawler_db"""
    if not MYSQL_URL:
        print("[MySQL] MYSQL_URL æœªè®¾ç½®ï¼Œè·³è¿‡ä¿å­˜")
        return

    posts = normalize_posts_to_content(posts)
    try:
        engine = create_engine(MYSQL_URL)
        with engine.begin() as conn:
            for post in posts:
                conn.execute(text("""
                    INSERT INTO reddit_posts 
                    (reddit_id, url, title, author, time, excerpt, content, fetched_at)
                    VALUES (:reddit_id, :url, :title, :author, :time, :excerpt, :content, :fetched_at)
                    ON DUPLICATE KEY UPDATE
                        title = VALUES(title),
                        author = VALUES(author),
                        time = VALUES(time),
                        excerpt = VALUES(excerpt),
                        content = VALUES(content),
                        fetched_at = VALUES(fetched_at)
                """), {
                    "reddit_id": post.get("id", ""),
                    "url": post.get("url", ""),
                    "title": post.get("title", ""),
                    "author": post.get("author", ""),
                    "time": post.get("time", "") or post.get("created_utc", ""),
                    "excerpt": post.get("excerpt", ""),
                    "content": post.get("content", ""),
                    "fetched_at": post.get("fetched_at", datetime.now().isoformat())
                })
        print(f"[MySQL] å·²ä¿å­˜ {len(posts)} æ¡å¸–å­åˆ° MySQL")
    except Exception as e:
        print(f"[MySQL] ä¿å­˜å¤±è´¥: {e}")


def load_posts_from_files(db_path: str = NEW_DB_PATH, csv_path: str = NEW_CSV_PATH) -> List[Dict]:
    """ä»æ–°DBæˆ–CSVè¯»å–å¸–å­ï¼ˆä¼˜å…ˆè¯»DBï¼ŒDBä¸å­˜åœ¨åˆ™è¯»CSVï¼‰"""
    posts = []
    # å°è¯•ä»SQLiteè¯»å–
    try:
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT * FROM posts", conn)
        conn.close()
        posts = df.to_dict("records")  # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        print(f"[load_posts_from_files] ä»DBè¯»å– {len(posts)} æ¡å¸–å­")
        return posts
    except Exception as e:
        print(f"[load_posts_from_files] ä»DBè¯»å–å¤±è´¥ï¼Œå°è¯•è¯»å–CSV: {e}")
    
    # DBè¯»å–å¤±è´¥åˆ™ä»CSVè¯»å–
    try:
        df = pd.read_csv(csv_path)
        posts = df.to_dict("records")
        print(f"[load_posts_from_files] ä»CSVè¯»å– {len(posts)} æ¡å¸–å­")
        return posts
    except Exception as e:
        print(f"[load_posts_from_files] ä»CSVè¯»å–å¤±è´¥: {e}")
        return []
    
def crawl_save_and_summarize(crawler, strategy, doubao_key, 
                             new_db=NEW_DB_PATH, new_csv=NEW_CSV_PATH):
    """å®Œæ•´æµç¨‹ï¼šçˆ¬å–å¸–å­â†’ä¿å­˜åˆ°æ–°æ–‡ä»¶â†’ç”Ÿæˆç»¼è¿°"""
    # æ­¥éª¤1ï¼šæ‰¹é‡çˆ¬å–æ‰€æœ‰æ–°å¸–å­ï¼ˆä½¿ç”¨ä½ ä¿®æ”¹åçš„get_new_postsï¼Œå¸¦æ—¶é—´è¿‡æ»¤ï¼‰
    try:
        print("\n===== å¼€å§‹æ‰¹é‡çˆ¬å–å¸–å­ =====")
        time_threshold = datetime.now() - timedelta(hours=24)   # ä½ çš„æ—¶é—´é˜ˆå€¼ï¼ˆå¦‚24å°æ—¶å‰ï¼‰
        all_new_posts = crawler.get_new_posts(
            subreddit_name=strategy["target_subreddit"],
            limit=10,
            max_comments=0,
            time_threshold=time_threshold.timestamp()
        )
        all_new_posts = normalize_posts_to_content(all_new_posts)
        print(f"[crawl_save_and_summarize] æœ¬æ¬¡çˆ¬å–åˆ° {len(all_new_posts)} æ¡å¸–å­ã€‚ç¬¬ä¸€æ¡ content ï¼š{all_new_posts[0].get('content','')}")
        if not all_new_posts:
            msg = f"===== æœªçˆ¬å–åˆ°æ–°å¸–å­ï¼Œæµç¨‹ç»ˆæ­¢ï¼ˆsubreddit={strategy['target_subreddit']}ï¼‰====="
            print(msg)
            return {
                "success": False,
                "message": msg,
                "summary": "",
                "data": []
            }
        
        # æ­¥éª¤2ï¼šä¿å­˜åˆ°æ–°çš„DBå’ŒCSV
        print("\n===== å¼€å§‹ä¿å­˜åˆ°æ–°æ–‡ä»¶ =====")
        for p in all_new_posts:
            p["content"] = p.get("content", "")
            p["excerpt"] = (p["content"][:150] + "...") if len(p["content"]) > 150 else p["content"]
        save_posts_to_sqlite(all_new_posts, db_path=new_db)
        save_posts_to_csv(all_new_posts, csv_path=new_csv)
        init_mysql_table()
        save_posts_to_mysql(all_new_posts)
        
        # æ­¥éª¤3ï¼šä»æ–°æ–‡ä»¶è¯»å–å¸–å­ï¼Œè°ƒç”¨è±†åŒ…ç”Ÿæˆç»¼è¿°
        print("\n===== å¼€å§‹ç”Ÿæˆç»¼åˆç»¼è¿° =====")
        posts_for_summary = load_posts_from_files(db_path=new_db, csv_path=new_csv)
        if not posts_for_summary:
            msg = "===== æ— å¸–å­å¯ç”Ÿæˆç»¼è¿° ====="
            print(msg)
            return {
                "success": False,
                "message": msg,
                "summary": "",
                "data": []
            }
        
        # è°ƒç”¨ä½ çš„summarize_with_doubaoå‡½æ•°ç”Ÿæˆç»¼è¿°
        summary_result = summarize_with_doubao(
            posts=posts_for_summary,
            doubao_key=doubao_key,
            model="doubao-1-5-pro-32k-250115"  # ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹å
        )
        
        summary_text = summary_result.get("summary", "").strip() or "ï¼ˆç»¼è¿°ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©ºï¼‰"
        with open("summary_result_test.txt", "w", encoding="utf-8") as f:
            f.write(summary_text)
        print("\n===== ç»¼è¿°ç”Ÿæˆå®Œæˆï¼Œå·²ä¿å­˜åˆ° summary_result_test.txt =====")
        print("ç»¼è¿°å†…å®¹ï¼š\n", summary_result["summary"])

        return {
            "success": True,
            "message": f"æˆåŠŸçˆ¬å– {len(all_new_posts)} æ¡å¸–å­å¹¶ç”Ÿæˆæ•´ä½“ç»¼è¿°",
            "summary": summary_text,
            "data": all_new_posts
            }

    except Exception as e:
        error_msg = f"crawl_save_and_summarize å‡ºé”™: {str(e)}"
        traceback.print_exc()
        return {
            "success": False,
            "message": error_msg,
            "summary": "",
            "data": []
        }
    
# ---------------------- 4. APIé‰´æƒ ----------------------
def verify_api_key(api_key: str = Query(..., description="APIè®¿é—®å¯†é’¥")):
    if api_key != API_CONFIG["api_key"]:
        raise HTTPException(status_code=401, detail="æ— æ•ˆçš„APIå¯†é’¥,è¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
    return api_key


# ---------------------- 5. APIè¯·æ±‚/å“åº”æ¨¡å‹ ----------------------
class IPAddRequest(BaseModel):
    ip: str
    protocol: Optional[str] = "http"

class StrategyUpdateRequest(BaseModel):
    concurrent_limit: Optional[int] = None
    crawl_interval: Optional[int] = None
    ip_switch_interval: Optional[int] = None
    retry_count: Optional[int] = None
    target_subreddit: Optional[str] = None
    max_posts_per_crawl: Optional[int] = None  # è¯¥å‚æ•°å®é™…å·²è¢«å›ºå®šä¸º10


# ---------------------- 6. APIç«¯ç‚¹å®ç° ----------------------
@app.get("/api/crawler/state", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def get_crawler_state_api(
    api_key: str = Depends(verify_api_key)
    ) -> Dict[str, Any]:
    return {
        "code": 200,
        "message": "success",
        "data": crawler_state.get_current_state()
    }

@app.post("/api/crawler/start", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def start_crawler_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    success = crawler_state.start_crawler()
    if success:
        try:
            summarize_result = crawl_save_and_summarize(
                crawler=RedditCrawler(),
                strategy=smart_strategy.get_current_strategy(),
                doubao_key=API_CONFIG["doubao_api_key"]
            )
            for post in summarize_result.get("posts", []):
                for item in post["data"]:
                    item.pop("selftext", None)
            return {
                "code": 200,
                "message": f"çˆ¬è™«å·²å¯åŠ¨å¹¶å®Œæˆæ‘˜è¦åŒ–å¤„ç†ï¼Œå…±å¤„ç† {len(summarize_result)} æ¡å¸–å­",
                "data": summarize_result
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"çˆ¬è™«å¯åŠ¨æˆåŠŸï¼Œä½†crawl_save_and_summarizeå¤±è´¥: {str(e)}")
    raise HTTPException(status_code=400, detail="çˆ¬è™«å·²åœ¨è¿›è¡Œä¸­ï¼Œæ— éœ€é‡å¤å¯åŠ¨")

@app.get("/api/crawler/results", tags=["1.çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def get_crawler_results(
    limit: int = Query(100, description="è¿”å›ç»“æœçš„æœ€å¤§æ•°é‡"),
    api_key = Depends(verify_api_key)
) -> Dict[str, Any]:
    results = crawler_state.get_crawled_posts(limit)
    return {
        "code": 200,
        "message": f"æˆåŠŸè·å– {len(results)} æ¡ç»“æœ",
        "data": {
            "posts": results,
            "total": len(results)
        }
    }

@app.post("/api/crawler/test-crawl", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def test_crawl_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    test_result = crawler_state.test_single_crawl()
    if test_result["success"]:
        return {
            "code": 200,
            "message": test_result["message"],
            "data": {
                "crawled_count": test_result["crawled_count"],
                "posts": test_result["posts"],
                "container_total": len(crawler_state.crawled_results)
            }
        }
    raise HTTPException(status_code=500, detail=test_result["message"])


@app.post("/api/crawler/stop", tags=["1. çˆ¬è™«çŠ¶æ€ç®¡ç†"])
def stop_crawler_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    success = crawler_state.stop_crawler()
    if success:
        return {
            "code": 200,
            "message": "çˆ¬è™«å·²åœæ­¢ï¼Œ å·²ä¿å­˜å¤„ç†è®°å½•",
            "data": {}
        }
    raise HTTPException(status_code=400, detail="çˆ¬è™«å·²åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ")

@app.get("/api/log", tags =["2. æ—¥å¿—ç®¡ç†"])
def get_logs_api(
    start_time: Optional[datetime] = Query(None, description= "æ—¥å¿—å¼€å§‹æ—¶é—´ï¼ˆå¦‚ 2025-09-19T12:00:00ï¼‰"),
    end_time: Optional[datetime] = Query(None, description="æ—¥å¿—ç»“æŸæ—¶é—´ï¼ˆå¦‚ 2025-09-19T12:00:00ï¼‰"),
    limit: int = Query(100, description="æœ€å¤šè¿”å›æ¡æ•°(<=1000)"),
    api_key: str = Depends(verify_api_key)
) ->Dict[str, Any]:
    limit = min(limit, 100)
    log_file = API_CONFIG["log_file_path"]

    if not os.path.exists(log_file):
        return {
            "code": 200,
            "message": "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨",
            "data": {"total": 0, "logs": []}
        }

    logs = []
    with open (log_file, "r", encoding="utf-8") as f:
        current_log = ""
        for line in f:
            if line.startswith("[") and len(line) >=20:
                if current_log:
                    logs.append(current_log.strip())
                current_log = line
            else:
                current_log += line
        if current_log:
            logs.append(current_log.strip())
    
    filtered_logs = []
    for log in logs:
        if not log.startswith("["):
            continue
        log_time_str = log[1:20]
        try:
            log_time = datetime.strptime(log_time_str,"%Y-%m-%d %H:%M:%S")
        except:
            continue

        if start_time and log_time < start_time:
            continue
        if end_time and log_time > end_time :
            continue
        filtered_logs.append(
            {
                "log_time": log_time_str,
                "content": log
            }
        )

    filtered_logs.sort(key=lambda x: x["log_time"], reverse=True)
    return {
        "code": 200,
        "message": "success",
        "data": {
            "total": len(filtered_logs),
            "limit": limit,
            "logs": filtered_logs[:limit]
        }
    }

@app.get("/api/anti-crawl/ip-pool", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-IPæ± "])
def get_ip_pool_status_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    return {
        "code": 200,
        "message": "success! æˆåŠŸè·å–åçˆ¬ç­–ç•¥",
        "data": ip_pool.get_pool_status()
    }

@app.post("/api/anti-crawl/ip-pool/add", tags=["3.åçˆ¬ç­–ç•¥ç®¡ç†-IPæ± "])
def add_ip_api(
    req: IPAddRequest,
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    if ":" not in req.ip:
        raise HTTPException(status_code=400, detail="IPæ ¼å¼é”™è¯¯ï¼Œéœ€ä¸º 'ip:port'ï¼ˆå¦‚ 1.2.3.4:7891ï¼‰")
    ip_parts = req.ip.split(":")
    if not ip_parts[1].isdigit():
        raise HTTPException(status_code=400, detail="IPç«¯å£å¿…é¡»ä¸ºæ•°å­—ï¼ˆå¦‚ 1.2.3.4:7891ï¼‰")
    
    success = ip_pool.add_ip(req.ip, req.protocol)
    if success:
        return {
            "code":200,
            "message": f"IP {req.ip} æ·»åŠ æˆåŠŸï¼ˆå·²éªŒè¯æœ‰æ•ˆï¼‰",
            "data": {}
        }
    raise HTTPException(status_code=400, detail=f"IP {req.ip} æ— æ•ˆæˆ–å·²åœ¨æ± ä¸­")

@app.post("/api/anti-crawl/ip-pool/remove", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-IPæ± "])
def remove_ip_api(
    ip: str = Query(..., description="å¾…åˆ é™¤IP(æ ¼å¼ï¼š ip:port)"),
    api_key: str = Depends(verify_api_key)
) ->  Dict[str, Any]:
    success = ip_pool.remove_ip(ip)
    if success:
        return {
            "code": 200,
            "message": f"IP {ip} å·²ä»æ± ä¸­åˆ é™¤",
            "data": {}
        }
    raise HTTPException(status_code=404, detail=f"IP {ip} ä¸åœ¨æ± ä¸­ï¼Œåˆ é™¤å¤±è´¥")

@app.get("/api/anti-crawl/strategy", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-æ™ºèƒ½å‚æ•°"])
def get_strategy_api(
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    return {
        "code": 200,
        "message": "success!æˆåŠŸè·å–æ¨¡å‹æ™ºèƒ½åçˆ¬ç­–ç•¥",
        "data": smart_strategy.get_current_strategy()
    }

@app.post("/api/anti-crawl/strategy/update", tags=["3. åçˆ¬ç­–ç•¥ç®¡ç†-æ™ºèƒ½å‚æ•°"])
def update_strategy_api(
    req: StrategyUpdateRequest,
    api_key: str = Depends(verify_api_key)
) -> Dict[str, Any]:
    new_params = req.model_dump(exclude_none=True)
    if not new_params:
        raise HTTPException(status_code=400, detail= "éœ€è‡³å°‘ä¼ å…¥ä¸€ä¸ªå¾…æ›´æ–°çš„ç­–ç•¥å‚æ•°")
    
    updated_strategy = smart_strategy.update_strategy(new_params)
    return {
        "code": 200,
        "message": "ç­–ç•¥æ›´æ–°æˆåŠŸ",
        "data": updated_strategy
    }

# ---------------------- 7. APIæœåŠ¡å¯åŠ¨å…¥å£ ----------------------
if __name__ == "__main__":
    print("="*60)
    print("         Redditçˆ¬è™«APIæœåŠ¡å¯åŠ¨ä¸­         ")
    print("="*60)
    print(f" APIåœ°å€ï¼šhttp://{API_CONFIG['host']}:{API_CONFIG['port']}")
    print(f" æ¥å£æ–‡æ¡£ï¼šhttp://{API_CONFIG['host']}:{API_CONFIG['port']}/docs")
    print(f" APIå¯†é’¥ï¼š{API_CONFIG['api_key']}ï¼ˆè¯·æ±‚æ—¶éœ€æºå¸¦ï¼‰")
    print(f" è±†åŒ…APIçŠ¶æ€ï¼š{'å·²åˆå§‹åŒ–' if doubao_client else 'åˆå§‹åŒ–å¤±è´¥'}")
    print(f" åçˆ¬æ¨¡å—ï¼šå·²åŠ è½½ IPæ± ï¼ˆåˆå§‹æœ‰æ•ˆIPï¼š{ip_pool.get_pool_status()['statistics']['valid_ip_count']}ï¼‰")
    print("="*60)
    
    uvicorn.run(
        app="__main__:app",
        host=API_CONFIG["host"],
        port=API_CONFIG["port"],
        workers=1,
        reload=False
    )

# if __name__ == "__main__":
#     print("="*60)
#     print("          Redditçˆ¬è™«åŠŸèƒ½æµ‹è¯•          ")
#     print("="*60)

#     # 1. åˆå§‹åŒ–æµ‹è¯•
#     print("\n[æµ‹è¯•1] ç»„ä»¶åˆå§‹åŒ–æ£€æŸ¥")
#     print(f"è±†åŒ…å®¢æˆ·ç«¯çŠ¶æ€: {'å·²å°±ç»ª' if doubao_client else 'æœªåˆå§‹åŒ–'}")
#     print(f"åˆå§‹IPæ± çŠ¶æ€: {ip_pool.get_pool_status()['statistics']}")
#     print(f"åˆå§‹åçˆ¬ç­–ç•¥: {smart_strategy.get_current_strategy()}")

#     print("\n[æµ‹è¯•1.5] crawl_save_and_summarize å®Œæ•´æµç¨‹æµ‹è¯•")
#     crawl_result = crawl_save_and_summarize(
#         crawler=RedditCrawler(), 
#         strategy=smart_strategy.get_current_strategy(),
#         doubao_key=API_CONFIG["doubao_api_key"],
#     )
#     print(f"æµ‹è¯•ç»“æœ: {crawl_result['message']}")
#     if crawl_result["success"]:
#         print("ç¬¬ä¸€æ¡å¸–å­æ ‡é¢˜:", crawl_result["data"][0]["title"])
#         print("ç»¼è¿°ç‰‡æ®µ:", crawl_result["summary"][:200])

#     # 2. æµ‹è¯•å•æ¬¡çˆ¬å–åŠŸèƒ½
#     print("\n[æµ‹è¯•2] å•æ¬¡çˆ¬å–æµ‹è¯•ï¼ˆæ— ä»£ç†ï¼‰")
#     test_result = crawler_state.test_single_crawl()
#     pprint({
#         "çˆ¬å–ç»“æœ": test_result["message"],
#         "è·å–æ•°é‡": test_result["crawled_count"]
#     })

#     # 3. æµ‹è¯•IPæ± æ“ä½œ
#     print("\n[æµ‹è¯•3] IPæ± åŠŸèƒ½æµ‹è¯•")
#     test_ip = "127.0.0.1:7897"
#     add_result = ip_pool.add_ip(test_ip, "http")
#     print(f"æ·»åŠ æµ‹è¯•IP {test_ip}: {'æˆåŠŸ' if add_result else 'å¤±è´¥'}")
#     print("æ›´æ–°åIPæ± çŠ¶æ€:", ip_pool.get_pool_status()["statistics"])
    
#     # 4. æµ‹è¯•åçˆ¬ç­–ç•¥æ›´æ–°
#     print("\n[æµ‹è¯•4] åçˆ¬ç­–ç•¥æ›´æ–°æµ‹è¯•")
#     new_strategy = {
#         "concurrent_limit": 5,
#         "crawl_interval": 10,
#         "target_subreddit": "python"  # æµ‹è¯•ç”¨å­ç‰ˆå—
#     }
#     updated = smart_strategy.update_strategy(new_strategy)
#     print("æ›´æ–°åç­–ç•¥:", updated)

#     # 5. æµ‹è¯•åå°çˆ¬è™«è¿è¡Œ
#     print("\n[æµ‹è¯•5] åå°çˆ¬è™«å¯åŠ¨æµ‹è¯•ï¼ˆæŒç»­15ç§’ï¼‰")
#     start_success = crawler_state.start_crawler()
#     if start_success:
#         print("çˆ¬è™«å¯åŠ¨æˆåŠŸï¼Œç­‰å¾…15ç§’...")
#         for i in range(3):
#             time.sleep(5)
#             print(f"\nè¿è¡Œ{5*(i+1)}ç§’åçŠ¶æ€:")
#             pprint(crawler_state.get_current_state()["basic_status"])
        
#         # 6. æµ‹è¯•ç»“æœè·å–
#         print("\n[æµ‹è¯•6] çˆ¬å–ç»“æœè·å–ä¸å­˜å‚¨")
#         results = crawler_state.get_crawled_posts(limit=5)
#         print(f"è·å–åˆ°{len(results)}æ¡ç»“æœï¼Œå‡†å¤‡ä¿å­˜åˆ°æ–‡ä»¶...")
#         if results:
#             # ä¿å­˜åˆ°SQLiteå’ŒCSVï¼ˆä¸è¡¨ç»“æ„å¯¹é½ï¼‰
#             save_posts_to_sqlite(results)
#             save_posts_to_csv(results)
#             print(f"å·²å°†{len(results)}æ¡ç»“æœä¿å­˜åˆ° {NEW_DB_PATH} å’Œ {NEW_CSV_PATH}")
#             print("ç¬¬ä¸€æ¡ä¿å­˜çš„æ ‡é¢˜:", results[0]["title"])
#         else:
#             print("æ— çˆ¬å–ç»“æœå¯ä¿å­˜")
        

#         # 7. åœæ­¢çˆ¬è™«
#         print("\n[æµ‹è¯•7] åœæ­¢çˆ¬è™«")
#         stop_success = crawler_state.stop_crawler()
#         print("çˆ¬è™«åœæ­¢:", "æˆåŠŸ" if stop_success else "å¤±è´¥")

#     # 8. æµ‹è¯•æ•°æ®å­˜å‚¨
#     print("\n[æµ‹è¯•8] æ•°æ®å­˜å‚¨æ£€æŸ¥")
#     saved_posts = load_posts_from_files()
#     print(f"ä»æ–‡ä»¶åŠ è½½åˆ°{len(saved_posts)}æ¡æ•°æ®")
#     if saved_posts:
#         print("å­˜å‚¨çš„ç¬¬ä¸€æ¡æ•°æ®URL:", saved_posts[0]["url"])
#         print("å­˜å‚¨å­—æ®µæ£€æŸ¥:", saved_posts[0].keys())

#     print("\n" + "="*60)
#     print("          æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæ¯•          ")
#     print("="*60)